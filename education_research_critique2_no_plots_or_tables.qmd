---
title: "Most Education Research is ***NOT*** Science"
subtitle: "When Ideology Masquerades as Scientific Inquiry"
description: ""
authors: 
  - name: "Dan Swart, CPA (ret)"
  - name: "Claude Sonnet 4.5"
date: today
date-format: long
# bibliography: manual-refs.bib
format:
  html:
    resources:
      - reference-backlinks.js
    include-after-body:    
      - text: |
          # <script type="text/javascript" src="reference-backlinks.js"></script>
    default: true         
    code-copy: true
    code-link: true        # This adds individual buttons
    code-fold: true        # Hide code by default, show on click
    code-summary: "Show the code"
    code-overflow: wrap
    code-block-bg: "#FAEBD7"
    code-block-border-left: "#31BAE9"
    embed-resources: true
    include-in-header:
      - text: 
          <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
          <link href="https://fonts.googleapis.com/css2?family=Fira+Mono&display=swap" rel="stylesheet">
          <link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet">
          <link href="https://fonts.googleapis.com/css2?family=Cabin&display=swap" rel="stylesheet">
          <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab&display=swap" rel="stylesheet">
          <link href="https://fonts.googleapis.com/css2?family=Schoolbell&display=swap" rel="stylesheet">
      - header.html
    css:
      - swart.css
      - tachyons.min.css
      - r-colors.css
    fontsize: 18pt
    lightbox: true
    page-layout: full
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    html-math-method: katex
    df-print: paged
    toc: true
    toc-float: true
    citeproc: true
    link-citations: true
    linestretch: 1.0
    
    
    
  typst:
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    margin:
      x: 1in
      y: 1in
    toc: true
    fontsize: 16pt
    mainfont: "Cabin"
  

    
  revealjs:
    slide-number: true
    transition: fade
    code-overflow: wrap
    center: true
    smaller: true
    scrollable: true
    chalkboard: true
    multiplex: false
    theme: solarized
    reference-location: margin
    logo: img/red-cross-640-435.png
    footer: "Footer text"
    code-block-height: 650px



  # docx:
  #   highlight-style: github
  #   fig_caption: true



editor: source

quarto:
  render:
    cache-refresh: true


# for .qmd filesd
execute:
  echo: true
  message: false
  warning: false
  eval: true
  fig-width: 12
  fig-height: 10


# for .rmd files
knitr:
  opts_chunk:
    echo: true
    error: false
    warning: false
    message: false
    cache: false


---


```{r}
#| label: setup
#| include: false


# install.packages(c("mapview", "survey", "srvyr", "arcgislayers"))

# census_api_key("95496766c51541ee6f402c1e1a8658581285b759", install = TRUE, overwrite = TRUE)


# # load libraries - NOT NEEDED


# Force dplyr's select to take precedence
select <- dplyr::select
filter <- dplyr::filter

# Options
options(scipen = 999)
options(qic.clshade = T) # NO LONGER NEEDED; CHARTS ALL PREPARED WITH GGPLOT2 ONLY
options(qic.linecol = 'black') # NO LONGER NEEDED; CHARTS ALL PREPARED WITH GGPLOT2 ONLY
options(qic.signalcol = "firebrick") # NO LONGER NEEDED; CHARTS ALL PREPARED WITH GGPLOT2 ONLY
options(qic.targetcol = "purple") # NO LONGER NEEDED; CHARTS ALL PREPARED WITH GGPLOT2 ONLY
options(DT.options = list(dom = 'pBlfrti')) # Add buttons, filtering, and top (t) pagination controls
options(shiny.maxRequestSize = 50 * 1024^2) # Set upload maximum to 50 MB
options(tigris_use_cache = TRUE)


# Flextable defaults:
flextable::set_flextable_defaults(
  font.size = 14, 
  font.family = "Cabin",
  font.color = "black",
  table.layout = "fixed",
  border.color = "darkgray",
  padding.top = 3, padding.bottom = 3,
  padding.left = 4, padding.right = 4,
  line_spacing = 1.3,
  digits = 2,
  decimal.mark = ".",
  big.mark = " ",
  na_str = "<na>",
  post_process_html = identity,
  post_process_docx = identity
)


#
# # Sample Code:
# flextable::flextable(violations) |>
 #  flextable::set_header_labels(
 #    Variable = "Variable",
 #    Measurement = "Measurement",
 #    Likely_Impact = "Likely Impact"
 #  ) |>
#    flextable::add_header_lines(values = "Frequent Violations of Scientific Method in Current So-Called 'Equity' Research") |>
#   flextable::color(i = 1, color = "blue", part = "header") |>
#   flextable::italic(i = 1, part = "header") |>
#   flextable::align(i = 1, align = "center", part = "header") |>
#   flextable::fontsize(i = 1, size = 14, part = "header") |>
#   flextable::bg(i = 1, bg = "white", part = "header") |>
#   flextable::bg(i = 2, bg = "palegreen", part = "header") |>
#   flextable::bold(i = 1:2, part = "header") |>
#   flextable::bold(i = 1:7, j = 1, part = "body") |>
#   ftExtra::colformat_md() |> 
#   flextable::autofit()
  

# Flextable built-in themes:
  # flextable::theme_alafoli()	|>  # BLAH
  # flextable::theme_apa()  # THIS IS NICE
  # flextable::theme_booktabs() |>  # NICE, MORE COMPACT
  # flextable::theme_box() |>   # OK, INCLUDES CELL BORDERS
  # flextable::theme_tron() |>  # 'DARK MODE' BLUE TEXT
  # flextable::theme_tron_legacy() |>   # 'DARK MODE' YELLOW TEXT
  # flextable::theme_vader() |>    # 'DARK MODE' WHITE TEXT
  # flextable::theme_vanilla() |>   # NOT SPECIAL
  # flextable::theme_zebra()	|>
  #

# Flextable titles:
  # flextable::colformat_double(j = c("Mean", "SD", "N"), big.mark = ",", digits = 1) |>
  # flextable::flextable(variance_comparison) |>
  #  flextable::add_header_lines(values = "The Within-Group vs Between-Group Variance Problem") |>

# Flextable Title theming at table-level:
  #  flextable::color(i = 1, color = "blue", part = "header") |>
  #  flextable::italic(i = 1, part = "header") |>
  #  flextable::align(i = 1, align = "center", part = "header") |>
  #  flextable::fontsize(i = 1, size = 14, part = "header") |>
  #  flextable::bg(i = 1, bg = "white", part = "header") |>

# Flextable standard table background colors:
  #  flextable::bg(i = 2, bg = "palegreen", part = "header") |>
  

# Flextable reading markdown:
  #  ftExtra::colformat_md() |> 

# Flextable auto-sizing cell widths:
  #  flextable::autofit() 
 
 # Flextable background based on SPECIFIC cell contents:
  #
  # flextable::bg(i = ~ Impact_on_Validity == "High", j = "Impact_on_Validity", bg = "#ffcccc") |>
 # flextable::bg(i = ~ Impact_on_Validity == "Medium", j = "Impact_on_Validity", bg = "#ffffcc") |>
  #
  # 
  # Apply yellow background to any cell containing "Yes":

  # for (col in base::names(hypotheses_data)) {
  #   yes_rows <- base::which(hypotheses_data[[col]] == "Yes")
  #   if (base::length(yes_rows) > 0) {
  #     ft <- ft |>
  #       flextable::bg(i = yes_rows, j = col, bg = "yellow", part = "body")
  #   }
  # } 
  #
  # Apply to last row of table:
  #
  #



# Set global theme for consistent plots
ggplot2::theme_set(
  ggplot2::theme_minimal(base_size = 20) +
    ggplot2::theme(
      plot.title.position = "plot",
      plot.title = ggtext::element_textbox_simple(
        family = "Cabin",
        face = "bold",
        color = "darkgreen",
        size = 26,
        fill = "yellow",
        lineheight = 1.2,
        padding = ggplot2::margin(5.5, 5.5, 0.0, 5.5),
        margin = ggplot2::margin(0, 0, 5.5, 0)
      ),
      plot.subtitle = ggtext::element_textbox_simple(
        family = "Cabin",
        color = "darkgreen",
        face = "bold",
        size = 24,
        fill = "yellow",
        lineheight = 1.2,
        padding = ggplot2::margin(5.5, 5.5, 5.5, 5.5),
        margin = ggplot2::margin(10.5, 0, 5.5, 0)
      ),
      plot.caption = ggtext::element_markdown(
        family = "Cabin",
        size = 22,
        hjust = 1,
        color = "darkblue",
        face = "italic",
        fill = "yellow",
        lineheight = 1.0
      ),
      axis.text.x = ggtext::element_markdown(
        family = "Cabin",
        face = "bold",
        color = "blue",
        size = 20,
        angle = 45,
        hjust = 1
      ),
        # ggplot2::element_blank(),
      axis.title.x = ggtext::element_markdown(
        family = "Cabin",
        face = "bold",
        color = "blue",
        size = 20),
        # ggplot2::element_blank(),
      axis.text.y = ggtext::element_markdown(
        family = "Cabin",
        face = "bold",
        color = "blue",
        size = 20,
        angle = 45,
        hjust = 1
      ),
        # ggplot2::element_blank(),
      axis.title.y = ggtext::element_markdown(
        family = "Cabin",
        face = "bold",
        color = "blue",
        size = 20),
        # ggplot2::element_blank(),
      strip.text = ggtext::element_markdown(
        family = "Cabin",
        color = "black",
        size = ggplot2::rel(1.1),
        face = "italic",
        margin = ggplot2::margin(2, 0, 0.5, 0, "lines")
      ),
      axis.text = ggtext::element_markdown(
        family = "Cabin",
        color = "black"),
      panel.background = ggplot2::element_rect(fill = "white", color = NA),
      plot.background = ggplot2::element_rect(fill = "white", color = NA),
      legend.position = "none",
      panel.spacing.x = grid::unit(1.5, "cm"),
      panel.spacing.y = grid::unit(1.5, "cm"),
      plot.margin = ggplot2::margin(20, 20, 20, 20, "pt")
    )
)



  
# Set seed for reproducibility
base::set.seed(123)

```


```{r}
#| label: libraries

# load libraries
library(tidyverse)
library(DT)
library(plotly)
library(ggplot2)
library(kableExtra)

```



### Executive Summary

Most educational "research" has become a crude form of confirmation bias that uses pseudo-scientific methodologies to validate predetermined beliefs, rather than discover or validate empirical facts. These pseudo-science methodologies undermine both scientific integrity and educational effectiveness, creating a system where ideology can trump evidence and advocacy masquerades as inquiry.

In general, research questions are poorly formulated, if they are formulated at all.  Causal assumptions are built-in to unverified premises, so no need to test and verify them empirically.  Statistics are calculated only to quantify the effects of the untested causal assumptions.  The assumptions themselves are not verified.  Rational subgrouping is ignored, in favor of treating groups as if every individual in the group can be characterized by the mean calculated for the group.  

Advise for action is to treat groups differently; never mind that many, many individuals in each group are far more similar to individuals in other groups than to individuals in their designated group.  There is never a search for results that challenge the findings.  There is never a search to quantify the effects of the system on results versus the effects of the individual on results.

Unfortunately, these practices are destructive and should be replaced.

**The solution requires acknowledging that too much current educational "research" serves ideological rather than scientific purposes. Serious education advocates within politics, academia, and the family must work to rebuild research practices around testable theory, hypothesis testing, replication requirements, and intellectual humility about what correlation calculations can actually tell us.**

The stakes are far too high and children's futures far too important to continue accepting pseudo-scientific advocacy as legitimate educational research.

---

<br>

### Background

Right now, teachers, administrators, and school boards are being offered 'solutions' labeled "evidence-based," "research-based," or "data-driven," and similar ambiguous terms designed to obscure the unscientific data analysis involved.  It is important to question these claims and be skeptical of them. This is not rejecting science but to stop education research from being influenced by political or personal beliefs.

This problem isn’t part of a plan to push one specific belief into the education system, though it can be used that way. In the 1960s and 1970s, exciting research in the social sciences gave the idea that human behavior could be understood scientifically (cause and effect). However, a key 2015 study showed that most social research results couldn’t be repeated, making it clear that there was a big problem in the way research is done. [@opensciencecollaboration2015]

In response, social science academics now focus more on technology (like programming and math) instead of keeping research methods strong. Universities have been teaching these weak methods for decades, and PhD committees often approve dissertations without questioning their lack of scientific rigor. So it's not really a conspiracy, it's cultural learning.  However, just because it is cultural learning does NOT mean it is benign.  In its current form, it is bad for all concerned.  As a result these methods have not provided new scientific knowledge or useful tools to help teachers or administrators improve their work.

---

<br>

### Current Practice

Right now, teachers, administrators, and school boards are being offered 'solutions' labeled "evidence-based," "research-based," or "data-driven," and similar ambiguous terms designed to obscure the unscientific data analysis involved.  It is important to question these claims and be skeptical of them. This is not rejecting science but to stop education research from being influenced by political or personal beliefs.

This problem isn’t part of a plan to push one specific belief into the education system, though it can be used that way. In the 1960s and 1970s, exciting research in the social sciences gave the idea that human behavior could be understood scientifically (cause and effect). However, a key 2015 study showed that most social research results couldn’t be repeated, making it clear that there was a big problem in the way research is done. [@opensciencecollaboration2015]

In response, social science academics now focus more on technology (like programming and math) instead of keeping research methods strong. Universities have been teaching these weak methods for decades, and PhD committees often approve dissertations without questioning their lack of scientific rigor. So it's not really a conspiracy, it's cultural learning.  Unfortunatly, just because it is cultural learning does NOT mean it is benign.  In its current form, it is destructive.  As a result these methods have not provided new scientific ideas or useful data to help teachers or administrators improve their work.

------------------------------------------------------------------------

### The Core Problem

Modern educational research is plagued by conclusions made before the research even starts, pretending to be real science. Instead of testing ideas to see if they’re empirically true, many researchers start with certain beliefs (like "equity", "systemic racism", or "school boards affect student success") and look for data to prove what they already believe. Even though many people call it "data science in education," I see no real science in it.

------------------------------------------------------------------------

### The Pseudo-Scientific Process

This ubiquitous research methodology follows a predictable pattern:

1.  **Start with ideological conviction** ("diffent results are evidence of systemic racism", "boards affect student achievement")

2.  **Assume causal mechanisms exist** without any theoretical or empirical justification

3.  **Mine data for supportive correlations** while ignoring contradictory evidence

4.  **Present correlational findings using causal language**

5.  **Implement policy based on weak or no causal evidence** that confirms pre-existing beliefs

------------------------------------------------------------------------


### Key Characteristics of Educational Pseudo-Research

#### **Correlation Hunting as 'Science'**

  -   Massive multiple comparisons without statistical correction
  
  -   Cherry-picking significant results from hundreds of tests; ignoring significant negative   correlations
  
  -   Treating correlation coefficients as effect sizes
  
  -   Using sophisticated statistical methods to obscure weak scientific logic

<br>

#### **Causal Language for Correlational Data**

  -   "X affects Y" when only correlation exists
  
  -   "Interventions work" based on observational studies
  
  -   Policy recommendations from associational findings
  
  -   Ignoring reverse causation and confounding variables

<br>

#### **Incentives for Confirmation Bias by Design**

  -   PhD candidates with personal missions to "make a difference"
  
  -   Studies designed to validate existing programs or beliefs
  
  -   Selective interpretation of results
  
  -   Post-hoc theorizing to explain desired findings

<br>

#### **Methodological Sophistication Masking Weak Science**
  
  -   Complex statistical procedures creating illusion of rigor
  
  -   Technical competence without scientific thinking
  
  -   Regression models that "control for" unmeasured constructs
  
  -   Effect size inflation through selective reporting

------------------------------------------------------------------------

<br>


### A Dissertation Example

In this PhD dissertation all of these practices come together in a 'perfect storm' of pseudo-scientific research. [@lorentzen2013relationship] The fact that this passed dissertation defense reflects broader problems with educational research standards and the willingness to accept loose correlation-based advocacy as legitimate scientific inquiry.  I have included my critique of the orignal research paper along with this one.

Here are just a few of his claims:

- **Correlation ≠ Causation Violations**

Despite repeatedly acknowledging this limitation, Lorentzen consistently violates this principle.


**Explicit Causal Language**:

- "School boards that accomplish the items identified in the BSAS **govern districts with the highest achievement scores**".  (Abstract)

- "The actions of school boards **matter**" (p. 113, emphasis added)

- "If student achievement is to continue to improve... all relevant factors **must be identified, employed, and aligned**" (p. 115)

- "Only the school board **can**, and **must**, accomplish certain needs vital to a successful school district" (p. 134).  (Thank G-d he's come along to tell us what those are)


**Problems**: These statements imply causal mechanisms based purely on correlational data. The research as planned and executed cannot distinguish whether:

- Good boards cause higher achievement

- Higher achievement attracts better board members

- Third variables cause both

- Relationships are entirely spurious

** No attempt to specify and control for confounders (not a DAG to be seen), which might have given him ***some*** standing to make at least weak causal claims.

Had the author, who obviously worked hard on the project, limited has claims to non-causal language and not called for wholesale changes by educators, I would have nothing to say about this work. 

Then again, no one else would either.

Unfortunately for all, this research spawned additional research with identical flaws.

<br>




### A "Woke" Example

Educational 'equity' research exemplifies this abandonment of the scientific method:

-   **Predetermined conclusion**: Systemic racism causes achievement gaps

-   **Data mining**: Finding group differences in outcomes as "evidence"

-   **Causal assumption**: Differences 'prove' discrimination without testing the premise or alternatives

-   **Policy prescription**: Calls for 'equity' interventions based on pseudo-scientific research

-   **Circular reasoning**: Failure of interventions blamed on "insufficient implementation"  


Here is an example of an educational textbook designed to teach data 'scientists' how to examine data using pseudo-scientific methods, searching for evidence pointing to institutional racism. [@bovee2024data]

The young, accomplished, and earnest authors deserve much praise for presenting a tour-de-force of R programming instruction. And, if that was as far as it went, my remarks would be limited to any perceived benefits of correlational studies to the practice of education. In fact, I would have little but praise.

However, the text is being used to educate young budding data 'scientists' who can't know why what they are learning is wrong. See Chapter 9.  For example, you can watch this video: [Data Science in Education Using R Book Club Chapter 9: Walkthrough 3 (2021-03-24) (dsieur01)](https://www.youtube.com/watch?v=Z5KRaOgW0sk&list=PL3x6DOfs2NGgvd741DNAnn1MEitDJQP3c&index=7)

------------------------------------------------------------------------

<br>

### The Institutional Problem

Pseudo-research thrives because:

#### **Academic Incentives Reward Advocacy, not Scientific Rigor**

  -   Publications favor studies confirming progressive assumptions
  
  -   "Null results" (no differences found) don't get published
  
  -   Career advancement requires supporting fashionable causes
  
  -   Peer review by ideologically aligned researchers

<br>

#### **Methodological Training Without Scientific Thinking**

  -   Students learn statistical procedures but not testing of theory
  
  -   Emphasis on finding "significant" results rather than testing theories
  
  -   Missing education in experimental design and causal inference
  
  -   Confusion between statistical significance and practical importance
  
  -   Students and instructors alike are ignorant of the crucial differences between enumerative studies, and analytic studies.   [Read about analytic and enumerative statistical studies here](https://en.wikipedia.org/wiki/Analytic_and_enumerative_statistical_studies){target="_blank"}



<br>

#### **Policy Implementation Without Standards of Evidence**

  -   Educational decisions based on weak correlational evidence
  
  -   Pressure for "evidence-based" practice accepts any published or unpublished study
  
  -   No requirement or expectation for replication or independent verification
  
  -   Resistance to criticism dismissed as bias or racist sentiment
  
  -   Foisted on those in the field of education least able to detect the scientific flows  


------------------------------------------------------------------------

<br>

### Real-World Consequences

This pseudo-scientific research causes genuine harm:

#### **Educational System Instability**

  -   Constant policy churn based on latest pseudo "research" findings
  
  -   Resources wasted on unproven, and often destructive interventions
  
  -   Teacher and administrator frustration with ineffective mandates
  
  -   Student outcomes suffer from system disruption
  
  -   Confusing common and special causes of variation leads to tampering, which leads to even more trouble

<br>

#### **Erosion of Public Trust**

  -   Parents observe disconnect between research claims and reality
  
  -   Professional educators lose faith in academic guidance
  
  -   Public skepticism of educational expertise grows
  
  -   Political polarization around educational issues increases

<br>

#### **Suppression of Genuine Inquiry**

  -   Questions about intervention effectiveness becomes taboo
  
  -   Alternative explanations for problems are not researched, or ignored
  
  -   Resources diverted from scientific approaches to fashionable ones
  
  -   Scientific method abandoned for ideological conformity

------------------------------------------------------------------------

<br>

### A Rational Alternative

W. Edwards Deming's approach offers a rational alternative:

  -   **Systems thinking** over individual blame/credit
  
  -   **Process improvement** based on understanding of variation
  
  -   **Operational definitions** of measurable outcomes
  
  -   **Continuous improvement** through mini-experimentation (the PDSA cycle)
  
  -   **Respect for complexity** rather than simplistic causal claims
  
  -   **Managing the Unknown and Unknowable** factors that cannot be measured ("if you cannot   measure it, you cannot manage it" is utter nonsense)
  
  -   **Understanding variation** and the difference between common and special causes
  
  -   **Understanding the difference** between enumerative and analytic studies

------------------------------------------------------------------------

<br>

### Recommendations for Reform

#### **Restore Scientific Standards**

  -   Require pre-registration of hypotheses and analysis plans
  
  -   Mandate replication studies before policy implementation
  
  -   Enforce correction for multiple testing
  
  -   Separate correlation from causation in language and policy
  
  -   Insist on testable theories that involve causation claims

<br>


#### **Reform Academic Incentives**

  -   Reward studies that fail to find expected effects
  
  -   Require disclosure of researcher advocacy positions
  
  -   Create independent review of education research claims
  
  -   Establish consequences for misleading causal claims

<br>


#### **Improve Practitioner Education**

  -   Train educators to critically evaluate research claims
  
  -   Teach dangers of confusing correlation and causation
  
  -   Teach difference between enumerative and analytic studies
  
  -   Teach that all management is prediction
  
  -   Emphasize effect sizes over statistical significance
  
  -   Promote healthy skepticism of research-based mandates
  
  -   Teach the dangers of confusing common cause variation with special cause variation
  
  -   Understand that systems thinking requires management to calculate (with data) the effects on performance created by the system vs the individual (Deming estimates that ratio to be about 96% system and 4% individual)


------------------------------------------------------------------------
