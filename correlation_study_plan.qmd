---
title: "Correlation Study Plan"
format:
  html:
    css: swart-20250630.css
    page-layout: full
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    df-print: paged
    code-overflow: wrap
    toc: true
  typst:
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    margin:
      x: 1in
      y: 1in
    toc: true
    fontsize: 14pt
    mainfont: "Latin Modern Roman"
execute:
  echo: false
  message: false
  warning: false
  fig-width: 12
  fig-height: 10
---


# Correlation Analysis: A Graduated Study Plan


*From Visual Intuition to Advanced Methods*

```{r}
#| label: setup

# Load required packages
library(tidyverse)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(ppcor)
library(car)
library(WRS2)
library(boot)
library(BayesFactor)
library(pwr)
library(qgraph)
library(patchwork)

# Set global theme for consistent plots
theme_set(theme_minimal(base_size = 16) + 
          theme(
    plot.title = element_text(face = "bold", size = 24),    # adjust title size
    plot.subtitle = element_text(face = "bold", size = 20), # adjust subtitle size
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    axis.text = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"),
    panel.spacing.x = unit(1.5, "cm"),  # Horizontal spacing only
    panel.spacing.y = unit(1.5, "cm"),   # Vertical spacing only
    plot.margin = margin(20, 20, 20, 20, "pt")
   )
  )

# Set seed for reproducibility
set.seed(42)

```

## Level 1: Visual Intuition (Foundation)

**Goal**: Develop pattern recognition for relationships

### Research Objectives & Practical Applications

**Why This Matters**: Before diving into numbers, you need to "see" relationships in data. This visual literacy prevents you from being fooled by correlation coefficients that don't match the actual pattern.

**Real-World Problems This Solves**:  

- **Quality Control**: Spotting when manufacturing processes go off-track  
- **Marketing**: Identifying if ad spend truly relates to sales increases  
- **Health Research**: Recognizing dose-response relationships in treatments  
- **Finance**: Detecting if two stocks actually move together or just coincidentally  

**Research Skills Developed**: Visual data exploration, pattern recognition, intuitive understanding of association strength  

### Core Concepts

- Scatter plots as correlation's "fingerprint"
- Positive vs negative relationships
- Linear vs non-linear patterns
- Strength of association (tight vs scattered clouds)

### R Practice

```{r}
#| label: level1-practice

# Create synthetic data for different correlation patterns
set.seed(123)

# Perfect positive correlation
perfect_pos <- tibble(
  x = 1:10,
  y = 1:10,
  type = "Perfect Positive (r = 1.0)"
)

# Perfect negative correlation
perfect_neg <- tibble(
  x = 1:10,
  y = 10:1,
  type = "Perfect Negative (r = -1.0)"
)

# Non-linear relationship
nonlinear <- tibble(
  x = 1:10,
  y = (1:10)^2,
  type = "Non-linear (r ≠ pattern strength)"
)

# No correlation
no_corr <- tibble(
  x = 1:10,
  y = rnorm(10, 5, 2),
  type = "No Correlation (r ≈ 0)"
)

# Combine datasets
correlation_patterns <- bind_rows(perfect_pos, perfect_neg, nonlinear, no_corr)

# Create visualization
p1 <- ggplot(correlation_patterns, aes(x = x, y = y)) +
  geom_point(color = "steelblue", size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, color = "red", alpha = 0.7) +
  facet_wrap(~type, scales = "free") +
  labs(title = "Visual Patterns of Correlation",
       subtitle = "Learning to 'see' relationships before calculating",
       x = "X Variable", 
       y = "Y Variable")

print(p1)

```



```{r}
#| label: level1-real-world-examples

# Real-world examples with synthetic data

# Quality Control: Temperature vs Defect Rate
qc_data <- tibble(
  temperature = seq(180, 220, length.out = 50),
  defect_rate = 15 - 0.3 * temperature + rnorm(50, 0, 2),
  scenario = "Quality Control"
)

# Marketing: Ad Spend vs Sales
marketing_data <- tibble(
  ad_spend = runif(50, 1000, 10000),
  sales = 50000 + 2.5 * ad_spend + rnorm(50, 0, 5000),
  scenario = "Marketing ROI"
)

# Health: Dose vs Response (non-linear)
health_data <- tibble(
  dose = seq(0, 10, length.out = 50),
  response = 100 * (1 - exp(-0.3 * dose)) + rnorm(50, 0, 5),
  scenario = "Dose-Response"
)

# Finance: Two correlated stocks
finance_data <- tibble(
  stock_a = cumsum(rnorm(50, 0.001, 0.02)),
  stock_b = 0.7 * stock_a + cumsum(rnorm(50, 0, 0.015)),
  day = 1:50,
  scenario = "Stock Correlation"
) %>%
  pivot_longer(cols = c(stock_a, stock_b), 
               names_to = "stock", values_to = "return")

# Create combined visualization
p2a <- ggplot(qc_data, aes(x = temperature, y = defect_rate)) +
  geom_point(color = "red", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  labs(title = "Quality Control", x = "Temperature (°C)", y = "Defect Rate (%)")

p2b <- ggplot(marketing_data, aes(x = ad_spend, y = sales)) +
  geom_point(color = "green", alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  labs(title = "Marketing ROI", x = "Ad Spend ($)", y = "Sales ($)")

p2c <- ggplot(health_data, aes(x = dose, y = response)) +
  geom_point(color = "blue", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "darkblue") +
  labs(title = "Dose-Response", x = "Drug Dose (mg)", y = "Response (%)")

p2d <- ggplot(finance_data, 
              aes(x = day, 
                  y = return, 
                  color = stock
                  )
              ) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c("purple", 
                                "orange"
                                )
                     ) +
  labs(title = "Stock Correlation", x = "Trading Day", y = "Cumulative Return",
       color = "Stock") 


# Combine plots & add annotation
combined_plot <- (p2a | p2b) / (p2c | p2d)

combined_plot <- combined_plot + 
  plot_annotation(
    title = "Real-World Correlation Patterns",
    subtitle = "Recognizing relationships across different domains"
  )

print(combined_plot)  

```


**Milestone**: Can look at any scatter plot and predict correlation direction and rough strength

<br>

## Level 2: Pearson Correlation Basics

**Goal**: Understand the standard correlation coefficient

### Research Objectives & Practical Applications

**Why This Matters**: Pearson's r is the "common language" of correlation - used in 90% of research papers. Without understanding it, you can't critically evaluate most scientific literature or business analytics reports.

**Real-World Problems This Solves**:  

- **Academic Research**: Writing papers that meet publication standards  
- **Business Analytics**: Quantifying relationships for stakeholder reports (e.g., "Customer satisfaction correlates r=0.73 with retention")  
- **Risk Management**: Measuring how closely different investments move together  
- **Survey Research**: Validating that questionnaire items measure the same construct  
- **Performance Evaluation**: Establishing if training programs actually improve results  

**Research Skills Developed**: Standardized effect size interpretation, basic statistical communication, hypothesis formation

### Core Concepts

- Pearson's r (-1 to +1 scale)
- Standardized measure (unitless)
- Linear relationships only
- Correlation ≠ causation

### R Practice

```{r}
#| label: level2-practice

# Create synthetic data for business scenarios
set.seed(456)

# Customer satisfaction vs retention
customer_data <- tibble(
  satisfaction = rnorm(100, 7, 2),
  retention_rate = 0.3 + 0.08 * satisfaction + rnorm(100, 0, 0.15)
) %>%
  mutate(retention_rate = pmax(0, pmin(1, retention_rate)))  # bound between 0 and 1

# Calculate correlation
cor_value <- cor(customer_data$satisfaction, customer_data$retention_rate)
cor_test_result <- cor.test(customer_data$satisfaction, customer_data$retention_rate)

# Visualization with correlation info
p3 <- ggplot(customer_data, aes(x = satisfaction, y = retention_rate)) +
  geom_point(alpha = 0.6, color = "steelblue", size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "red", alpha = 0.7) +
  labs(
    title = paste("Customer Satisfaction vs Retention Rate"),
    subtitle = paste("Pearson r =", round(cor_value, 3), 
                     "| p-value =", round(cor_test_result$p.value, 4)),
    x = "Satisfaction Score (1-10)",
    y = "Retention Rate (0-1)"
  ) +
  annotate("text", x = 3, y = 0.9, 
           label = paste("Interpretation:\nModerate positive correlation\n",
                        "r =", round(cor_value, 3), "suggests satisfaction\n",
                        "explains ~", round(cor_value^2 * 100, 1), "% of retention variance"),
           hjust = 0, size = 3.5, color = "darkblue")

print(p3)
```



```{r}
#| label: level2-strength-examples
# Demonstrate different correlation strengths
set.seed(789)

# Create data with different correlation strengths
create_corr_data <- function(n = 50, r_target, name) {
  x <- rnorm(n)
  y <- r_target * x + sqrt(1 - r_target^2) * rnorm(n)
  tibble(x = x, y = y, 
         correlation = paste(name, "(r =", round(cor(x, y), 2), ")"))
}

# Generate different strength examples
weak_pos <- create_corr_data(50, 0.2, "Weak")
moderate_pos <- create_corr_data(50, 0.5, "Moderate") 
strong_pos <- create_corr_data(50, 0.8, "Strong")

correlation_strengths <- bind_rows(weak_pos, moderate_pos, strong_pos)

# Visualization
p4 <- ggplot(correlation_strengths, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, color = "darkgreen", size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(~correlation) +
  labs(
    title = "Correlation Strength Visualization",
    subtitle = "Learning to distinguish weak, moderate, and strong relationships",
    x = "X Variable", y = "Y Variable"
  ) +
  theme(strip.text = element_text(face = "bold"))

print(p4)
```



```{r}
#| label: level2-business-applications
# Business applications with interpretation
set.seed(101)

# Employee training hours vs performance
training_data <- tibble(
  training_hours = runif(80, 5, 40),
  performance_score = 60 + 0.8 * training_hours + rnorm(80, 0, 8),
  department = sample(c("Sales", "Engineering", "Marketing"), 80, replace = TRUE)
)

cor_training <- cor(training_data$training_hours, training_data$performance_score)

p5 <- ggplot(training_data, aes(x = training_hours, y = performance_score)) +
  geom_point(aes(color = department), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "black", alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    title = "Training Investment vs Performance",
    subtitle = paste("r =", round(cor_training, 3), 
                     "- Strong positive correlation suggests ROI on training"),
    x = "Training Hours", 
    y = "Performance Score",
    color = "Department"
  ) +
  theme(legend.position = "bottom")

print(p5)
```

### Key Interpretations

- |r| < 0.3: weak
- |r| 0.3-0.7: moderate  
- |r| > 0.7: strong

**Milestone**: Comfortable calculating and interpreting basic correlation coefficients

<br>

## Level 3: Statistical Inference

**Goal**: Determine if correlations are "real" or just random noise

### Research Objectives & Practical Applications

**Why This Matters**: Anyone can find correlations in data by chance. This level teaches you to distinguish between meaningful relationships and statistical flukes - critical for making sound decisions based on data.

**Real-World Problems This Solves**:  

- **Clinical Trials**: Determining if treatment effects are real or just random variation  
- **Market Research**: Avoiding costly business decisions based on spurious correlations  
- **A/B Testing**: Knowing when you have enough data to make reliable conclusions  
- **Academic Integrity**: Meeting peer review standards for statistical rigor  
- **Investment Decisions**: Understanding when historical correlations are reliable predictors  
- **Policy Making**: Ensuring public health or economic policies are based on solid evidence  

**Research Skills Developed**: Hypothesis testing logic, p-value interpretation, confidence interval reasoning, sample size planning  

### Core Concepts

- Null hypothesis testing for correlation
- p-values and confidence intervals
- Sample size effects
- Multiple testing corrections

### R Practice

```{r}
#| label: level3-practice

# Statistical inference for correlation with visualization
set.seed(202)

# Clinical trial: Treatment dose vs recovery time
clinical_data <- tibble(
  dose_mg = runif(60, 10, 100),
  recovery_days = 20 - 0.12 * dose_mg + rnorm(60, 0, 3)
) %>%
  mutate(recovery_days = pmax(1, recovery_days))  # minimum 1 day

# Perform correlation test
cor_test <- cor.test(clinical_data$dose_mg, clinical_data$recovery_days)

# Create visualization with confidence information
p6 <- ggplot(clinical_data, aes(x = dose_mg, y = recovery_days)) +
  geom_point(alpha = 0.7, color = "darkred", size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", alpha = 0.8) +
  labs(
    title = "Clinical Trial: Drug Dose vs Recovery Time",
    subtitle = paste("r =", round(cor_test$estimate, 3), 
                     "| 95% CI: [", round(cor_test$conf.int[1], 3), ",", 
                     round(cor_test$conf.int[2], 3), "]",
                     "| p =", round(cor_test$p.value, 4)),
    x = "Drug Dose (mg)", 
    y = "Recovery Time (days)"
  ) +
  annotate("text", x = 70, y = 18, 
           label = paste("Statistical Significance:\n",
                        "p <", ifelse(cor_test$p.value < 0.001, "0.001", "0.05"),
                        "\nThis correlation is unlikely\ndue to chance alone"),
           size = 3.5, color = "darkblue", hjust = 0)

print(p6)
```

```{r}
#| label: level3-sample-size-effects
# Demonstrate sample size effects on correlation reliability
set.seed(303)

# Function to simulate correlation with different sample sizes
simulate_correlation <- function(n, true_r = 0.5) {
  x <- rnorm(n)
  y <- true_r * x + sqrt(1 - true_r^2) * rnorm(n)
  cor_test <- cor.test(x, y)
  
  tibble(
    n = n,
    observed_r = cor_test$estimate,
    ci_lower = cor_test$conf.int[1],
    ci_upper = cor_test$conf.int[2],
    p_value = cor_test$p.value,
    significant = cor_test$p.value < 0.05
  )
}

# Simulate across different sample sizes
sample_sizes <- c(10, 25, 50, 100, 200)
sample_size_results <- map_dfr(sample_sizes, ~simulate_correlation(.x))

# Visualization of sample size effects
p7 <- ggplot(sample_size_results, aes(x = n, y = observed_r)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_point(aes(color = significant), size = 4) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper, color = significant), 
                width = 5, size = 1) +
  scale_color_manual(values = c("FALSE" = "gray", "TRUE" = "darkgreen")) +
  labs(
    title = "Sample Size Effects on Correlation Estimates",
    subtitle = "True correlation = 0.5 (red dashed line)",
    x = "Sample Size (n)", 
    y = "Observed Correlation (r)",
    color = "Statistically\nSignificant"
  ) +
  theme(legend.position = "right")

print(p7)
```

```{r}
#| label: level3-power-analysis
# Power analysis visualization
# Calculate power for different sample sizes and effect sizes
power_data <- expand_grid(
  n = seq(10, 200, by = 10),
  effect_size = c(0.1, 0.3, 0.5, 0.7)
) %>%
  mutate(
    power = map2_dbl(n, effect_size, ~pwr.r.test(n = .x, r = .y)$power),
    effect_label = paste("r =", effect_size)
  )

p8 <- ggplot(power_data, aes(x = n, y = power, color = effect_label)) +
  geom_line(linewidth = 1.2) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red") +
  scale_color_viridis_d() +
  labs(
    title = "Statistical Power Analysis for Correlation",
    subtitle = "Power to detect correlation at α = 0.05 (80% power threshold shown)",
    x = "Sample Size (n)", 
    y = "Statistical Power",
    color = "True Effect Size"
  ) +
  theme(legend.position = "right")

print(p8)
```

**Milestone**: Can assess statistical significance and understand confidence in correlation estimates

<br>

## Level 4: Alternative Correlation Methods

**Goal**: Handle non-normal data and non-linear relationships

### Research Objectives & Practical Applications

**Why This Matters**: Real-world data rarely meets Pearson correlation's assumptions. This level prevents you from missing important relationships or drawing wrong conclusions from messy, real data.

**Real-World Problems This Solves**:  

- **Survey Research**: Analyzing Likert scale responses (ordinal data) properly  
- **Medical Research**: Handling skewed biomarkers or ranked results  
- **Environmental Studies**: Detecting monotonic relationships in ecological data  
- **Social Sciences**: Working with ranked preferences or income data  
- **Machine Learning**: Feature selection with mixed data types  
- **Quality Control**: Monitoring non-linear process relationships  
- **Customer Analytics**: Understanding satisfaction rankings vs continuous metrics  

**Research Skills Developed**: Assumption checking, method selection based on data characteristics, non-parametric thinking  

### Core Concepts

- **Spearman's rank correlation**: for ordinal data or non-linear monotonic relationships
- **Kendall's tau**: robust to outliers, better for small samples
- **Point-biserial**: one continuous, one binary variable
- **Phi coefficient**: both variables binary

### R Practice

### Choosing the Right Correlation Method

*Pearson vs Spearman vs Kendall for different data types*

```{r}
#| label: level4-practice


# Demonstrate different correlation methods with synthetic data
set.seed(404)

# Create different types of relationships
# 1. Linear relationship (good for Pearson)
linear_data <- tibble(
  x = seq(1, 20, length.out = 50),
  y = 2 * x + rnorm(50, 0, 5),
  relationship = "Linear"
)

# 2. Monotonic non-linear (good for Spearman)
monotonic_data <- tibble(
  x = seq(1, 20, length.out = 50),
  y = x^1.5 + rnorm(50, 0, 10),
  relationship = "Monotonic Non-linear"
)

# 3. Ordinal/ranked data simulation
set.seed(505)
ordinal_responses <- tibble(
  customer_id = 1:100,
  satisfaction_rank = sample(1:5, 100, replace = TRUE, prob = c(0.1, 0.15, 0.3, 0.35, 0.1)),
  loyalty_rank = pmax(1, pmin(5, satisfaction_rank + sample(-2:2, 100, replace = TRUE))),
  relationship = "Ordinal Data"
)

# Calculate different correlations
linear_cors <- tribble(
  ~method, ~correlation,
  "Pearson", cor(linear_data$x, linear_data$y, method = "pearson"),
  "Spearman", cor(linear_data$x, linear_data$y, method = "spearman"),
  "Kendall", cor(linear_data$x, linear_data$y, method = "kendall")
)

monotonic_cors <- tribble(
  ~method, ~correlation,
  "Pearson", cor(monotonic_data$x, monotonic_data$y, method = "pearson"),
  "Spearman", cor(monotonic_data$x, monotonic_data$y, method = "spearman"),
  "Kendall", cor(monotonic_data$x, monotonic_data$y, method = "kendall")
)

ordinal_cors <- tribble(
  ~method, ~correlation,
  "Pearson", cor(ordinal_responses$satisfaction_rank, ordinal_responses$loyalty_rank, method = "pearson"),
  "Spearman", cor(ordinal_responses$satisfaction_rank, ordinal_responses$loyalty_rank, method = "spearman"),
  "Kendall", cor(ordinal_responses$satisfaction_rank, ordinal_responses$loyalty_rank, method = "kendall")
)

# Create visualizations
p9a <- ggplot(linear_data, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, color = "blue", size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Linear Relationship", 
       subtitle = paste("Pearson r =", round(linear_cors$correlation[1], 3),
                       "| Spearman ρ =", round(linear_cors$correlation[2], 3)),
       x = "X", y = "Y")

p9b <- ggplot(monotonic_data, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, color = "green", size = 2) +
  geom_smooth(method = "loess", se = FALSE, color = "red") +
  labs(title = "Monotonic Non-linear", 
       subtitle = paste("Pearson r =", round(monotonic_cors$correlation[1], 3),
                       "| Spearman ρ =", round(monotonic_cors$correlation[2], 3)),
       x = "X", y = "Y")

p9c <- ggplot(ordinal_responses, aes(x = factor(satisfaction_rank), y = factor(loyalty_rank))) +
  geom_count(alpha = 0.7, color = "purple") +
  scale_size_area(max_size = 8) +
  labs(title = "Ordinal Data", 
       subtitle = paste("Pearson r =", round(ordinal_cors$correlation[1], 3),
                       "| Spearman ρ =", round(ordinal_cors$correlation[2], 3)),
       x = "Satisfaction Rank", y = "Loyalty Rank",
       size = "Count")


print(p9a)
print(p9b)
print(p9c)

```

```{r}
#| label: level4-medical-example
# Medical research example: biomarker levels (skewed data)
set.seed(606)

# Simulate biomarker data (often skewed in medical research)
biomarker_data <- tibble(
  patient_id = 1:80,
  # Log-normal distribution for biomarker (common in medicine)
  biomarker_level = rlnorm(80, meanlog = 2, sdlog = 0.8),
  # Treatment response related to biomarker but with noise
  treatment_response = pmax(0, pmin(100, 30 + 15 * log(biomarker_level) + rnorm(80, 0, 10)))
)

# Calculate correlations
bio_pearson <- cor(biomarker_data$biomarker_level, biomarker_data$treatment_response)
bio_spearman <- cor(biomarker_data$biomarker_level, biomarker_data$treatment_response, method = "spearman")

# Create visualization
p10 <- ggplot(biomarker_data, aes(x = biomarker_level, y = treatment_response)) +
  geom_point(alpha = 0.7, color = "darkred", size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "blue", alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE, color = "red", linetype = "dashed") +
  labs(
    title = "Biomarker Level vs Treatment Response",
    subtitle = paste("Pearson r =", round(bio_pearson, 3), 
                     "| Spearman ρ =", round(bio_spearman, 3),
                     "\nSpearman better captures monotonic relationship"),
    x = "Biomarker Level (ng/mL)", 
    y = "Treatment Response (%)",
    caption = "Blue = linear fit, Red dashed = non-linear fit"
  )

print(p10)

```


```{r}
#| label: level4-survey-research
# Survey research with Likert scales
set.seed(707)

# Simulate Likert scale responses (1-7 scale)
survey_data <- tibble(
  respondent = 1:200,
  job_satisfaction = sample(1:7, 200, replace = TRUE, 
                           prob = c(0.05, 0.1, 0.15, 0.2, 0.25, 0.15, 0.1)),
  work_engagement = pmax(1, pmin(7, job_satisfaction + sample(-2:2, 200, replace = TRUE)))
)

# Calculate correlations
survey_pearson <- cor(survey_data$job_satisfaction, survey_data$work_engagement)
survey_spearman <- cor(survey_data$job_satisfaction, survey_data$work_engagement, method = "spearman")

# Create visualization showing discrete nature
p11 <- ggplot(survey_data, aes(x = factor(job_satisfaction), y = factor(work_engagement))) +
  geom_count(alpha = 0.7, color = "darkgreen") +
  scale_size_area(max_size = 10) +
  labs(
    title = "Job Satisfaction vs Work Engagement (Likert Scales)",
    subtitle = paste("Pearson r =", round(survey_pearson, 3), 
                     "| Spearman ρ =", round(survey_spearman, 3),
                     "\nSpearman preferred for ordinal data"),
    x = "Job Satisfaction (1-7 scale)", 
    y = "Work Engagement (1-7 scale)",
    size = "Response\nCount"
  ) 
  # theme(legend.position = "right")

print(p11)
```

### When to Use What

Think of these as different tools for different data "textures":

- **Pearson**: Linear relationships, normal data
- **Spearman**: Monotonic relationships, ordinal data
- **Kendall**: Small samples, many tied values

**Milestone**: Can choose appropriate correlation method based on data characteristics


<br>

## Level 5: Partial and Semi-Partial Correlations

**Goal**: Control for confounding variables

### Research Objectives & Practical Applications

**Why This Matters**: Most real-world relationships involve multiple variables. This level helps you isolate the specific relationship you care about while controlling for confounding factors - essential for causal inference.

**Real-World Problems This Solves**:  

- **Medical Research**: Isolating treatment effects while controlling for age, gender, baseline health  
- **Educational Research**: Measuring teaching effectiveness while controlling for student background  
- **Economic Analysis**: Understanding wage-education relationships while controlling for experience  
- **Marketing Attribution**: Determining which advertising channels truly drive sales  
- **HR Analytics**: Identifying what really predicts job performance beyond obvious factors  
- **Environmental Studies**: Separating climate effects from other environmental factors  
- **Social Research**: Understanding pure relationships between variables of interest  

**Research Skills Developed**: Confound identification, variable selection, causal reasoning, advanced statistical modeling preparation  

### Core Concepts

- Partial correlation: relationship between X and Y controlling for Z
- Semi-partial: unique contribution of X to Y
- Multiple correlation: relationship between Y and multiple X variables


### R Practice

```{r}
#| label: level5-practice

# Demonstrate partial correlation with confounding variables
set.seed(808)

# Educational research scenario: Student performance data
education_data <- tibble(
  student_id = 1:150,
  # Confounding variable: socioeconomic status (affects both study time and performance)
  socioeconomic_status = rnorm(150, 50, 15),
  # Study time is influenced by socioeconomic status
  study_hours = pmax(0, 10 + 0.3 * socioeconomic_status + rnorm(150, 0, 8)),
  # Test performance influenced by both study time AND socioeconomic status
  test_score = pmax(0, pmin(100, 
                           30 + 1.2 * study_hours + 0.4 * socioeconomic_status + rnorm(150, 0, 10)))
)

# Calculate different types of correlations
simple_cor <- cor(education_data$study_hours, education_data$test_score)
partial_cor_result <- pcor(education_data[c("study_hours", "test_score", "socioeconomic_status")])
partial_cor <- partial_cor_result$estimate[1, 2]

# Visualization showing the confounding effect
p12a <- ggplot(education_data, aes(x = study_hours, y = test_score)) +
  geom_point(aes(color = socioeconomic_status), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "black") +
  scale_color_viridis_c(name = "Socioeconomic\nStatus") +
  labs(
    title = "Simple Correlation: Study Hours vs Test Score",
    subtitle = paste("r =", round(simple_cor, 3), 
                     "(includes confounding effect of socioeconomic status)"),
    x = "Study Hours per Week", 
    y = "Test Score"
  )

# Show partial correlation effect
p12b <- education_data %>%
  mutate(ses_group = cut(socioeconomic_status, breaks = 3, 
                        labels = c("Low SES", "Medium SES", "High SES"))) %>%
  ggplot(aes(x = study_hours, y = test_score)) +
  geom_point(aes(color = ses_group), alpha = 0.7, size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(~ses_group) +
  scale_color_viridis_d() +
  labs(
    title = "Partial Correlation: Controlling for Socioeconomic Status",
    subtitle = paste("Partial r =", round(partial_cor, 3), 
                     "(pure study-performance relationship)"),
    x = "Study Hours per Week", 
    y = "Test Score",
    color = "SES Group"
  ) +
  theme(legend.position = "none", strip.text = element_text(face = "bold"))

combined_partial <- p12a / p12b
combined_partial + plot_annotation(
  title = "Understanding Confounding Variables",
  subtitle = "How partial correlation reveals true relationships"
)

```

```{r}
#| label: level5-medical-confounding

# Medical research: Treatment effectiveness controlling for baseline health
set.seed(909)

medical_data <- tibble(
  patient_id = 1:120,
  # Baseline health affects both treatment dose and result
  baseline_health = rnorm(120, 70, 15),
  # Treatment dose adjusted based on patient health
  treatment_dose = pmax(10, pmin(100, 
                                50 + 0.2 * baseline_health + rnorm(120, 0, 10))),
  # Recovery result depends on both dose and baseline health
  recovery_score = pmax(0, pmin(100,
                               20 + 0.4 * treatment_dose + 0.3 * baseline_health + rnorm(120, 0, 12)))
)

# Calculate correlations
simple_med_cor <- cor(medical_data$treatment_dose, medical_data$recovery_score)
partial_med_result <- pcor(medical_data[c("treatment_dose", "recovery_score", "baseline_health")])
partial_med_cor <- partial_med_result$estimate[1, 2]

# Visualization
p13 <- ggplot(medical_data, aes(x = treatment_dose, y = recovery_score)) +
  geom_point(aes(color = baseline_health), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "black", alpha = 0.3) +
  scale_color_gradient2(low = "red", mid = "yellow", high = "green", 
                       midpoint = 70, name = "Baseline\nHealth") +
  labs(
    title = "Treatment Dose vs Recovery (Medical Research)",
    subtitle = paste("Simple r =", round(simple_med_cor, 3), 
                     "| Partial r (controlling baseline) =", round(partial_med_cor, 3)),
    x = "Treatment Dose (mg)", 
    y = "Recovery Score",
    caption = "Color shows baseline health confounding effect"
  )

print(p13)

```


```{r}
#| label: level5-hr-analytics
# HR Analytics: Job performance controlling for experience
set.seed(1010)

hr_data <- tibble(
  employee_id = 1:100,
  years_experience = runif(100, 0, 20),
  # Training hours influenced by experience (experienced employees get more training)
  training_hours = pmax(5, 20 + 2 * years_experience + rnorm(100, 0, 15)),
  # Performance influenced by both training and experience
  performance_rating = pmax(1, pmin(10,
                                   3 + 0.1 * training_hours + 0.15 * years_experience + rnorm(100, 0, 1.5)))
)

# Calculate correlations
hr_simple <- cor(hr_data$training_hours, hr_data$performance_rating)
hr_partial_result <- pcor(hr_data[c("training_hours", "performance_rating", "years_experience")])
hr_partial <- hr_partial_result$estimate[1, 2]

# Create visualization showing the unique contribution of training
p14 <- ggplot(hr_data, aes(x = training_hours, y = performance_rating)) +
  geom_point(aes(color = years_experience), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "darkblue") +
  scale_color_viridis_c(name = "Years\nExperience") +
  labs(
    title = "Training Hours vs Performance (HR Analytics)",
    subtitle = paste("Simple r =", round(hr_simple, 3), 
                     "| Partial r (controlling experience) =", round(hr_partial, 3),
                     "\nPartial correlation shows pure training effect"),
    x = "Training Hours", 
    y = "Performance Rating (1-10)"
  )

print(p14)

```


**Analogy**: Like isolating one voice in a crowded room by filtering out background noise

**Milestone**: Can disentangle complex multi-variable relationships


<br>

## Level 6: Correlation Matrices and Visualization

**Goal**: Analyze many variables simultaneously

### Research Objectives & Practical Applications

**Why This Matters**: Modern datasets often contain dozens or hundreds of variables. This level teaches you to find patterns in high-dimensional data and communicate complex relationships effectively to stakeholders.

**Real-World Problems This Solves**:  

- **Genomics**: Identifying co-expressed genes in large gene expression datasets
- **Financial Portfolio Management**: Understanding correlations between many assets simultaneously
- **Customer Segmentation**: Finding patterns across multiple customer behaviors
- **Survey Validation**: Checking if questionnaire items cluster as expected
- **Quality Control**: Monitoring relationships between multiple process parameters
- **Market Research**: Understanding how many product features relate to each other
- **Neuroscience**: Analyzing brain connectivity patterns across many regions
- **Climate Science**: Understanding relationships between multiple environmental variables

**Research Skills Developed**: High-dimensional data visualization, pattern recognition in complex data, effective scientific communication

### Core Concepts

- Correlation matrices
- Heatmaps and correlograms
- Hierarchical clustering of variables
- Network analysis of correlations

### R Practice

```{r}
#| label: level6-practice


# Correlation matrix analysis with multiple variables
set.seed(1111)

# Customer analytics dataset with multiple behavioral variables
customer_analytics <- tibble(
  customer_id = 1:200,
  website_visits = rpois(200, 15),
  email_opens = rpois(200, 8),
  social_engagement = rpois(200, 5),
  purchase_frequency = rpois(200, 3),
  customer_service_calls = rpois(200, 2),
  satisfaction_score = rnorm(200, 7, 1.5),
  loyalty_score = rnorm(200, 6, 2),
  revenue = rlnorm(200, 6, 1)
)

# Calculate correlation matrix
cor_matrix <- customer_analytics %>%
  dplyr::select(-customer_id) %>%
  cor()

# Create correlation heatmap
p15 <- ggcorrplot(cor_matrix, 
                  hc.order = TRUE,
                  type = "lower",
                  lab = TRUE,
                  lab_size = 3,
                  method = "circle",
                  colors = c("red", "white", "blue"),
                  title = "Customer Analytics Correlation Matrix",
                  ggtheme = theme_minimal())

print(p15)

```

```{r}
#| label: level6-financial-portfolio


# Financial portfolio correlation analysis
set.seed(1212)

# Simulate stock returns for portfolio analysis
n_days <- 252  # trading days in a year
stock_names <- c("Tech_Stock", "Finance_Stock", "Healthcare_Stock", 
                "Energy_Stock", "Consumer_Stock", "Industrial_Stock")

# Create correlated stock returns
base_market <- rnorm(n_days, 0.0005, 0.02)  # market factor

portfolio_data <- tibble(
  day = 1:n_days,
  Tech_Stock = 0.8 * base_market + rnorm(n_days, 0, 0.015),
  Finance_Stock = 0.6 * base_market + rnorm(n_days, 0, 0.018),
  Healthcare_Stock = 0.4 * base_market + rnorm(n_days, 0, 0.012),
  Energy_Stock = 0.7 * base_market + rnorm(n_days, 0, 0.025),
  Consumer_Stock = 0.5 * base_market + rnorm(n_days, 0, 0.014),
  Industrial_Stock = 0.6 * base_market + rnorm(n_days, 0, 0.016)
)

# Calculate correlation matrix
portfolio_cor <- portfolio_data %>%
  dplyr::select(-day) %>%
  cor()

# Create professional correlation plot
p16a <- corrplot(portfolio_cor, 
                method = "color",
                type = "upper",
                order = "hclust",
                tl.cex = 0.8,
                tl.col = "black",
                addCoef.col = "black",
                number.cex = 0.7,
                title = "Portfolio Correlation Analysis\n",
                mar = c(0,0,2,0))

# Network visualization of strong correlations
library(qgraph)
# Only show correlations above 0.3
strong_cors <- portfolio_cor
strong_cors[abs(strong_cors) < 0.3] <- 0

p16b <- qgraph(strong_cors, 
               layout = "spring",
               vsize = 8,
               labels = stock_names,
               label.cex = 1,
               edge.width = 3,
               minimum = 0.3,
               title = "Portfolio Network (r > 0.3)")

# Time series visualization
portfolio_long <- portfolio_data %>%
  tidyr::pivot_longer(cols = -day, names_to = "stock", values_to = "return") %>%
  dplyr::group_by(stock) %>%
  dplyr::mutate(cumulative_return = cumsum(return))

p16c <- ggplot(portfolio_long, aes(x = day, y = cumulative_return, color = stock)) +
  geom_line(linewidth = 1) +
  scale_color_viridis_d() +
  labs(
    title = "Cumulative Returns: Portfolio Correlation in Action",
    subtitle = "Correlated stocks move together, especially during market stress",
    x = "Trading Day", 
    y = "Cumulative Return",
    color = "Stock"
  ) +
  theme(legend.position = "bottom")

print(p16c)

```

```{r}
#| label: level6-genomics-example

# Genomics: Gene expression correlation analysis
set.seed(1313)

# Simulate gene expression data for pathway analysis
n_samples <- 50
gene_names <- paste0("Gene_", LETTERS[1:12])

# Create gene expression with some co-expressed groups
# Group 1: Metabolic pathway (Genes A-D)
metabolic_factor <- rnorm(n_samples, 0, 1)
# Group 2: Immune response (Genes E-H)  
immune_factor <- rnorm(n_samples, 0, 1)
# Group 3: Cell cycle (Genes I-L)
cellcycle_factor <- rnorm(n_samples, 0, 1)

gene_expression <- tibble(
  sample_id = 1:n_samples,
  Gene_A = 5 + 2 * metabolic_factor + rnorm(n_samples, 0, 0.5),
  Gene_B = 4 + 1.8 * metabolic_factor + rnorm(n_samples, 0, 0.6),
  Gene_C = 6 + 1.5 * metabolic_factor + rnorm(n_samples, 0, 0.7),
  Gene_D = 5.5 + 1.7 * metabolic_factor + rnorm(n_samples, 0, 0.5),
  Gene_E = 7 + 2.2 * immune_factor + rnorm(n_samples, 0, 0.8),
  Gene_F = 6.5 + 1.9 * immune_factor + rnorm(n_samples, 0, 0.6),
  Gene_G = 8 + 2.1 * immune_factor + rnorm(n_samples, 0, 0.7),
  Gene_H = 7.2 + 2.0 * immune_factor + rnorm(n_samples, 0, 0.5),
  Gene_I = 4.5 + 1.6 * cellcycle_factor + rnorm(n_samples, 0, 0.6),
  Gene_J = 5.2 + 1.8 * cellcycle_factor + rnorm(n_samples, 0, 0.7),
  Gene_K = 4.8 + 1.7 * cellcycle_factor + rnorm(n_samples, 0, 0.5),
  Gene_L = 5.5 + 1.9 * cellcycle_factor + rnorm(n_samples, 0, 0.6)
)

# Calculate gene correlation matrix
gene_cor <- gene_expression %>%
  dplyr::select(-sample_id) %>%
  cor()

# Create clustered heatmap showing pathway groups
p17 <- ggcorrplot(gene_cor,
                  hc.order = TRUE,
                  type = "full",
                  lab = TRUE,
                  lab_size = 2.5,
                  colors = c("blue", "white", "red"),
                  title = "Gene Expression Correlation Matrix\n
                  Clustering reveals co-expressed pathway groups"
                  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p17)

```

**Milestone**: Can explore and visualize complex multi-dimensional relationships


<br>

## Level 7: Advanced Diagnostics and Assumptions

**Goal**: Understand when correlations might be misleading

### Research Objectives & Practical Applications

**Why This Matters**: Correlations can be deeply misleading when assumptions are violated. This level teaches you to be a "correlation detective" - spotting when relationships aren't what they seem and avoiding costly analytical mistakes.

**Real-World Problems This Solves**:  

- **Financial Risk Management**: Detecting when correlations break down during market stress
- **Clinical Research**: Identifying when patient subgroups have different treatment responses
- **Business Analytics**: Spotting when aggregate trends hide important subgroup patterns
- **Manufacturing**: Detecting when outlier conditions distort process relationships
- **Marketing**: Understanding when demographic targeting masks true customer segments
- **Epidemiology**: Identifying when population-level relationships don't hold for individuals
- **Environmental Monitoring**: Detecting when measurement errors or extreme events distort relationships

**Research Skills Developed**: Critical thinking about data, assumption checking, robust analytical practices, advanced diagnostic skills

### Core Concepts

- **Outlier influence**: How single points can distort correlations
- **Restriction of range**: Truncated data underestimates correlation
- **Curvilinear relationships**: When Pearson's r misses the pattern
- **Simpson's paradox**: Correlations that reverse when grouped

### R Practice

```{r}
#| label: level7-practice
# Outlier influence demonstration
set.seed(1414)

# Create dataset with and without outliers
normal_data <- tibble(
  x = rnorm(30, 50, 10),
  y = 20 + 0.8 * x + rnorm(30, 0, 5),
  outlier_status = "Normal Data"
)

# Add influential outliers
outlier_data <- normal_data %>%
  add_row(x = 85, y = 25, outlier_status = "With Outliers") %>%
  add_row(x = 15, y = 75, outlier_status = "With Outliers") %>%
  mutate(outlier_status = "With Outliers")

# Calculate correlations
normal_cor <- cor(normal_data$x, normal_data$y)
outlier_cor <- cor(outlier_data$x, outlier_data$y)

# Visualization showing outlier impact
combined_data <- bind_rows(
  normal_data,
  outlier_data
)

p18 <- ggplot(combined_data, aes(x = x, y = y)) +
  geom_point(aes(color = ifelse(outlier_status == "With Outliers" & (x > 80 | y > 70), 
                               "Outlier", "Normal")), 
             size = 2.5, alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.3) +
  facet_wrap(~outlier_status) +
  scale_color_manual(values = c("Normal" = "blue", "Outlier" = "red")) +
  labs(
    title = "Outlier Influence on Correlation",
    subtitle = paste("Normal data r =", round(normal_cor, 3), 
                     "| With outliers r =", round(outlier_cor, 3)),
    x = "X Variable", 
    y = "Y Variable",
    color = "Point Type"
  ) +
  theme(strip.text = element_text(face = "bold"))

print(p18)
```


```{r}
#| label: level7-restriction-of-range
# Restriction of range demonstration
set.seed(1515)

# Full range data
full_range <- tibble(
  student_id = 1:200,
  sat_score = rnorm(200, 1200, 200),
  gpa = pmax(0, pmin(4.0, 1.5 + 0.002 * sat_score + rnorm(200, 0, 0.4)))
)

# Restricted range (only high-performing students)
restricted_range <- full_range %>%
  filter(sat_score > 1300)  # Only top students

# Calculate correlations
full_cor <- cor(full_range$sat_score, full_range$gpa)
restricted_cor <- cor(restricted_range$sat_score, restricted_range$gpa)

# Combined visualization
p19a <- ggplot(full_range, aes(x = sat_score, y = gpa)) +
  geom_point(alpha = 0.6, color = "blue", size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Full Range",
    subtitle = paste("r =", round(full_cor, 3)),
    x = "SAT Score", y = "GPA"
  )

p19b <- ggplot(restricted_range, aes(x = sat_score, y = gpa)) +
  geom_point(alpha = 0.6, color = "green", size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(
    title = "Restricted Range (SAT > 1300)",
    subtitle = paste("r =", round(restricted_cor, 3)),
    x = "SAT Score", y = "GPA"
  )

restriction_plot <- p19a | p19b
restriction_plot + plot_annotation(
  title = "Restriction of Range Effects",
  subtitle = "How limiting data range artificially reduces correlation"
)
```


```{r}
#| label: level7-simpsons-paradox
# Simpson's Paradox demonstration
set.seed(1616)

# Create data showing Simpson's Paradox
hospital_data <- tibble(
  # Hospital A: treats sicker patients but has better doctors
  hospital_a_severity = rnorm(100, 80, 10),  # higher severity
  hospital_a_recovery = pmax(20, pmin(100, 90 - 0.6 * hospital_a_severity + rnorm(100, 0, 8))),
  hospital = "Hospital A"
) %>%
  rename(severity = hospital_a_severity, recovery = hospital_a_recovery) %>%
  bind_rows(
    tibble(
      # Hospital B: treats healthier patients with average doctors
      severity = rnorm(100, 40, 10),  # lower severity
      recovery = pmax(20, pmin(100, 70 - 0.6 * severity + rnorm(100, 0, 8))),
      hospital = "Hospital B"
    )
  )

# Calculate correlations
overall_cor <- cor(hospital_data$severity, hospital_data$recovery)
hospital_a_cor <- cor(filter(hospital_data, hospital == "Hospital A")$severity,
                     filter(hospital_data, hospital == "Hospital A")$recovery)
hospital_b_cor <- cor(filter(hospital_data, hospital == "Hospital B")$severity,
                     filter(hospital_data, hospital == "Hospital B")$recovery)

# Visualization
p20 <- ggplot(hospital_data, aes(x = severity, y = recovery)) +
  geom_point(aes(color = hospital), alpha = 0.7, size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linewidth = 1.5) +  # overall trend
  geom_smooth(aes(color = hospital), method = "lm", se = FALSE, linetype = "dashed") +  # within-group trends
  scale_color_manual(values = c("Hospital A" = "red", "Hospital B" = "blue")) +
  labs(
    title = "Simpson's Paradox in Medical Data",
    subtitle = paste("Overall r =", round(overall_cor, 3), 
                     "| Hospital A r =", round(hospital_a_cor, 3),
                     "| Hospital B r =", round(hospital_b_cor, 3)),
    x = "Patient Severity Score", 
    y = "Recovery Score",
    color = "Hospital",
    caption = "Black line = overall trend, Dashed = within-hospital trends"
  )

print(p20)
```



```{r}
#| label: level7-curvilinear-relationships


# Curvilinear relationship that Pearson misses
set.seed(1717)

# U-shaped relationship (performance vs stress)
stress_performance <- tibble(
  stress_level = runif(100, 0, 10),
  # Inverted U: performance peaks at moderate stress
  performance = 50 + 20 * stress_level - 2 * stress_level^2 + rnorm(100, 0, 8)
) %>%
  mutate(performance = pmax(0, pmin(100, performance)))

# Calculate correlations
linear_cor <- cor(stress_performance$stress_level, stress_performance$performance)

# Fit polynomial model for comparison
poly_model <- lm(performance ~ poly(stress_level, 2), data = stress_performance)
r_squared <- summary(poly_model)$r.squared

# Visualization
p21 <- ggplot(stress_performance, aes(x = stress_level, y = performance)) +
  geom_point(alpha = 0.7, color = "purple", size = 2.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +  # linear
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = TRUE, color = "blue") +  # quadratic
  labs(
    title = "Curvilinear Relationship: Stress vs Performance",
    subtitle = paste("Pearson r =", round(linear_cor, 3), 
                     "| Polynomial R² =", round(r_squared, 3),
                     "\nLinear correlation misses the inverted-U pattern"),
    x = "Stress Level", 
    y = "Performance Score",
    caption = "Red dashed = linear fit, Blue = quadratic fit"
  )

print(p21)
```

**Milestone**: Can diagnose problematic correlations and understand their limitations


<br>


## Level 8: Modern Robust Methods

**Goal**: Handle real-world messy data

### Research Objectives & Practical Applications

**Why This Matters**: This is the "advanced practitioner" level - you can now handle the messy, imperfect data that exists in the real world. These methods are what separate amateur from professional data analysis.

**Real-World Problems This Solves**:  

- **Big Data Analytics**: Handling datasets with inevitable outliers and measurement errors  
- **Longitudinal Studies**: Managing missing data and varying measurement conditions  
- **Cross-Cultural Research**: Accounting for different response patterns across populations  
- **Sensor Data Analysis**: Dealing with equipment malfunctions and environmental interference  
- **Financial Modeling**: Creating robust models that work during market anomalies  
- **Medical Diagnostics**: Developing reliable tests that work across diverse patient populations  
- **Climate Research**: Analyzing data from multiple sources with different reliability levels  
- **Social Media Analytics**: Handling noisy, unstructured behavioral data  

**Research Skills Developed**: Advanced statistical methodology, uncertainty quantification, robust analytical thinking, cutting-edge research methods  

### Core Concepts

- **Robust correlations**: Resistant to outliers (Winsorized, trimmed)
- **Bootstrap confidence intervals**: Non-parametric uncertainty estimates
- **Permutation tests**: Distribution-free significance testing
- **Bayesian correlation**: Incorporating prior knowledge

### R Practice

```{r}
#| label: level8-practice
# Robust correlation methods for messy real-world data
set.seed(1818)

# Create dataset with outliers and measurement errors
messy_data <- tibble(
  subject_id = 1:80,
  # Base relationship with noise
  x_clean = rnorm(80, 50, 15),
  y_clean = 30 + 0.7 * x_clean + rnorm(80, 0, 8)
) %>%
  mutate(
    # Add measurement errors and outliers
    x_messy = ifelse(row_number() <= 5, x_clean + rnorm(5, 0, 40), x_clean),
    y_messy = ifelse(row_number() <= 5, y_clean + rnorm(5, 0, 30), y_clean),
    outlier_status = ifelse(row_number() <= 5, "Outlier", "Normal")
  )

# Calculate different correlation methods
pearson_clean <- cor(messy_data$x_clean, messy_data$y_clean)
pearson_messy <- cor(messy_data$x_messy, messy_data$y_messy)
winsorized_cor <- wincor(messy_data$x_messy, messy_data$y_messy)$cor

# Visualization comparing methods
p22a <- ggplot(messy_data, aes(x = x_clean, y = y_clean)) +
  geom_point(alpha = 0.7, color = "blue", size = 2) +
  geom_smooth(method = "lm", se = TRUE, color = "darkblue") +
  labs(
    title = "Clean Data",
    subtitle = paste("Pearson r =", round(pearson_clean, 3)),
    x = "X Variable", y = "Y Variable"
  )

p22b <- ggplot(messy_data, aes(x = x_messy, y = y_messy)) +
  geom_point(aes(color = outlier_status), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "red", alpha = 0.3) +
  scale_color_manual(values = c("Normal" = "blue", "Outlier" = "red")) +
  labs(
    title = "Messy Data with Outliers",
    subtitle = paste("Pearson r =", round(pearson_messy, 3), 
                     "| Winsorized r =", round(winsorized_cor, 3)),
    x = "X Variable", y = "Y Variable",
    color = "Data Type"
  ) +
  theme(legend.position = "bottom")

robust_comparison <- p22a | p22b
robust_comparison + plot_annotation(
  title = "Robust vs Traditional Correlation Methods",
  subtitle = "How robust methods handle outliers and measurement errors"
)
```



```{r}
#| label: level8-bootstrap-confidence
# Bootstrap confidence intervals for correlation
set.seed(1919)

# Climate data with seasonal patterns and measurement uncertainties
climate_data <- tibble(
  month = rep(1:12, 5),  # 5 years of monthly data
  temperature = 15 + 10 * sin(2 * pi * (month - 3) / 12) + rnorm(60, 0, 3),
  co2_concentration = 400 + 0.5 * temperature + rnorm(60, 0, 5)
)

# Bootstrap function for correlation
boot_correlation <- function(data, indices) {
  sample_data <- data[indices, ]
  cor(sample_data$temperature, sample_data$co2_concentration)
}

# Perform bootstrap
boot_result <- boot(climate_data, boot_correlation, R = 2000)

# Calculate confidence intervals
boot_ci <- boot.ci(boot_result, type = "perc")

# Regular correlation test for comparison
regular_cor_test <- cor.test(climate_data$temperature, climate_data$co2_concentration)

# Visualization
p23a <- ggplot(climate_data, aes(x = temperature, y = co2_concentration)) +
  geom_point(aes(color = factor(month)), alpha = 0.7, size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "black") +
  scale_color_viridis_d(name = "Month") +
  labs(
    title = "Temperature vs CO2 Concentration",
    subtitle = paste("Bootstrap 95% CI: [", round(boot_ci$percent[4], 3), ",", 
                     round(boot_ci$percent[5], 3), "]",
                     "\nParametric 95% CI: [", round(regular_cor_test$conf.int[1], 3), ",",
                     round(regular_cor_test$conf.int[2], 3), "]"),
    x = "Temperature (°C)", 
    y = "CO2 Concentration (ppm)"
  ) +
  theme(legend.position = "right")

# Bootstrap distribution
bootstrap_df <- tibble(correlation = boot_result$t[,1])

p23b <- ggplot(bootstrap_df, aes(x = correlation)) +
  geom_histogram(bins = 50, alpha = 0.7, fill = "skyblue", color = "darkblue") +
  geom_vline(xintercept = boot_ci$percent[4:5], color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean(bootstrap_df$correlation), color = "darkred", size = 1) +
  labs(
    title = "Bootstrap Distribution of Correlation",
    subtitle = "Red dashed lines show 95% confidence interval",
    x = "Correlation Coefficient", 
    y = "Frequency"
  )

bootstrap_analysis <- p23a / p23b
bootstrap_analysis + plot_annotation(
  title = "Bootstrap Confidence Intervals for Correlation",
  subtitle = "Non-parametric uncertainty estimation"
)

```

```{r}
#| label: level8-bayesian-correlation

# Bayesian correlation analysis
set.seed(2020)

# Sensor data with some uncertainty
sensor_data <- tibble(
  measurement_id = 1:50,
  sensor_a = rnorm(50, 100, 20),
  sensor_b = 0.6 * sensor_a + rnorm(50, 20, 15)
)

# Bayesian correlation test
bayes_cor_result <- correlationBF(sensor_data$sensor_a, sensor_data$sensor_b)

# Extract Bayes factor
bf_value <- extractBF(bayes_cor_result)$bf[1]

# Traditional correlation for comparison
traditional_cor <- cor.test(sensor_data$sensor_a, sensor_data$sensor_b)

# Visualization
p24 <- ggplot(sensor_data, aes(x = sensor_a, y = sensor_b)) +
  geom_point(alpha = 0.7, color = "darkgreen", size = 3) +
  geom_smooth(method = "lm", se = TRUE, color = "purple") +
  labs(
    title = "Bayesian vs Frequentist Correlation Analysis",
    subtitle = paste("Traditional r =", round(traditional_cor$estimate, 3),
                     "| p =", round(traditional_cor$p.value, 4),
                     "\nBayes Factor =", round(bf_value, 2), 
                     "(", ifelse(bf_value > 3, "Strong", ifelse(bf_value > 1, "Moderate", "Weak")), 
                     " evidence for correlation)"),
    x = "Sensor A Reading", 
    y = "Sensor B Reading",
    caption = "Bayes Factor > 3 = strong evidence, 1-3 = moderate, < 1 = weak"
  )

print(p24)
```



```{r}
#| label: level8-permutation-test
# Permutation test for correlation
set.seed(2121)

# Social media data: engagement vs reach (potentially non-normal)
social_media <- tibble(
  post_id = 1:60,
  reach = rexp(60, rate = 1/1000),  # exponential distribution
  engagement = 50 + 0.02 * reach + rexp(60, rate = 1/20)  # also skewed
)

# Observed correlation
observed_cor <- cor(social_media$reach, social_media$engagement)

# Permutation test function
permutation_test <- function(x, y, n_perm = 1000) {
  observed <- cor(x, y)
  
  # Generate null distribution by permuting y
  null_cors <- map_dbl(1:n_perm, ~{
    y_permuted <- sample(y)
    cor(x, y_permuted)
  })
  
  # Calculate p-value
  p_value <- mean(abs(null_cors) >= abs(observed))
  
  list(
    observed = observed,
    null_distribution = null_cors,
    p_value = p_value
  )
}

# Run permutation test
perm_result <- permutation_test(social_media$reach, social_media$engagement, 2000)

# Traditional test for comparison
traditional_test <- cor.test(social_media$reach, social_media$engagement)

# Visualization
p25a <- ggplot(social_media, aes(x = reach, y = engagement)) +
  geom_point(alpha = 0.7, color = "orange", size = 2.5) +
  geom_smooth(method = "lm", se = TRUE, color = "darkblue") +
  labs(
    title = "Social Media: Reach vs Engagement",
    subtitle = paste("Observed r =", round(observed_cor, 3),
                     "\nPermutation p =", round(perm_result$p_value, 4),
                     "| Traditional p =", round(traditional_test$p.value, 4)),
    x = "Reach (thousands)", 
    y = "Engagement Rate"
  )

# Null distribution from permutation test
null_df <- tibble(correlation = perm_result$null_distribution)

p25b <- ggplot(null_df, aes(x = correlation)) +
  geom_histogram(bins = 50, alpha = 0.7, fill = "lightblue", color = "darkblue") +
  geom_vline(xintercept = observed_cor, color = "red", size = 2) +
  geom_vline(xintercept = -observed_cor, color = "red", size = 2, linetype = "dashed") +
  labs(
    title = "Permutation Test: Null Distribution",
    subtitle = "Red line shows observed correlation",
    x = "Correlation Under Null Hypothesis", 
    y = "Frequency"
  )

permutation_analysis <- p25a / p25b
permutation_analysis + plot_annotation(
  title = "Permutation Test for Correlation",
  subtitle = "Distribution-free significance testing"
)

```



```{r}
#| label: level8-longitudinal-robust

# Longitudinal data with missing values and varying conditions
set.seed(2222)

# Patient monitoring over time with some missing data
longitudinal_data <- tibble(
  patient_id = rep(1:20, each = 12),  # 20 patients, 12 months
  month = rep(1:12, 20),
  # Blood pressure with individual differences and time trends
  systolic_bp = rep(rnorm(20, 130, 20), each = 12) + 
                rep(rnorm(20, 0, 5), each = 12) * month + 
                rnorm(240, 0, 8),
  # Medication adherence related to BP control
  adherence_score = pmax(0, pmin(100, 
                                80 - 0.3 * (systolic_bp - 130) + rnorm(240, 0, 15))),
  # Introduce some missing data (realistic in longitudinal studies)
  missing_indicator = rbinom(240, 1, 0.15)
) %>%
  mutate(
    systolic_bp = ifelse(missing_indicator == 1, NA, systolic_bp),
    adherence_score = ifelse(missing_indicator == 1, NA, adherence_score)
  ) %>%
  filter(!is.na(systolic_bp) & !is.na(adherence_score))

# Calculate robust correlation accounting for patient clustering
# Using complete cases
robust_longitudinal_cor <- cor(longitudinal_data$systolic_bp, 
                              longitudinal_data$adherence_score, 
                              method = "spearman")

# Visualization showing individual patient trajectories
p26 <- ggplot(longitudinal_data, aes(x = systolic_bp, y = adherence_score)) +
  geom_point(aes(color = factor(patient_id)), alpha = 0.6, size = 1.5) +
  geom_smooth(method = "lm", se = TRUE, color = "black", linewidth = 1.5) +
  facet_wrap(~cut(month, breaks = c(0, 4, 8, 12), labels = c("Months 1-4", "Months 5-8", "Months 9-12"))) +
  scale_color_viridis_d(guide = "none") +  # Too many patients for legend
  labs(
    title = "Longitudinal Patient Data: Blood Pressure vs Medication Adherence",
    subtitle = paste("Spearman ρ =", round(robust_longitudinal_cor, 3),
                     "| n =", nrow(longitudinal_data), "observations from 20 patients"),
    x = "Systolic Blood Pressure (mmHg)", 
    y = "Medication Adherence Score (%)",
    caption = "Each color represents a different patient across time periods"
  ) +
  theme(strip.text = element_text(face = "bold"))

print(p26)

```



### Bayesian Correlation

```{r}
#| label: bayesian-correlation
library(BayesFactor)

# Bayesian correlation test
x_clean <- rnorm(50)
y_clean <- 0.4 * x_clean + rnorm(50, 0, 0.8)

bayes_cor <- correlationBF(x_clean, y_clean)
bayes_cor
```

**Milestone**: Can handle non-ideal data with sophisticated methods

<br>

## Study Strategy

### Week-by-Week Approach

- **Weeks 1-2**: Levels 1-2 (build intuition)
- **Weeks 3-4**: Level 3 (statistical foundations)
- **Weeks 5-6**: Level 4 (method variety)
- **Weeks 7-8**: Level 5 (advanced concepts)
- **Weeks 9-10**: Levels 6-7 (practical applications)
- **Weeks 11-12**: Level 8 (cutting-edge methods)

### Practice Projects

- **Level 3**: Analyze relationship between height/weight
- **Level 5**: Economics data with multiple predictors
- **Level 6**: Gene expression or financial data
- **Level 8**: Climate data with outliers and missing values

<br>

### Key Resources

```{r}
#| label: resources
#| eval: false
# Essential packages to install
install.packages(c(
  "tidyverse", "ggplot2", "corrplot", "ggcorrplot", "ppcor", "car", 
  "WRS2", "boot", "BayesFactor", "pwr", "qgraph", "patchwork"
))
```
<br>

### Practice Projects by Level

**Level 3 Practice**: Analyze relationship between height/weight in a health dataset
```{r}
#| label: practice-project-3
#| eval: false
# Generate synthetic health data
health_study <- tibble(
  participant_id = 1:200,
  height_cm = rnorm(200, 170, 10),
  weight_kg = 0.8 * height_cm - 60 + rnorm(200, 0, 8)
) %>%
  mutate(weight_kg = pmax(40, weight_kg))  # realistic minimum weight

# Your task: Calculate correlation, test significance, determine sample size needed
```

<br>

**Level 5 Practice**: Economics data with multiple predictors
```{r}
#| label: practice-project-5
#| eval: false
# Generate economic indicators
economic_data <- tibble(
  country_id = 1:50,
  gdp_per_capita = rlnorm(50, 9, 1),
  education_spending = 3 + 0.5 * log(gdp_per_capita) + rnorm(50, 0, 1),
  healthcare_spending = 2 + 0.3 * log(gdp_per_capita) + rnorm(50, 0, 0.8),
  life_expectancy = 60 + 8 * log(gdp_per_capita) + 2 * education_spending + rnorm(50, 0, 3)
)

# Your task: Use partial correlation to isolate education's effect on life expectancy
```

<br>

**Level 6 Practice**: Gene expression or financial data
```{r}
#| label: practice-project-6
#| eval: false
# Generate stock portfolio data
portfolio_analysis <- tibble(
  date = seq.Date(from = as.Date("2023-01-01"), by = "day", length.out = 252)
) %>%
  mutate(
    market_factor = rnorm(252, 0.001, 0.02),
    tech_stock = 0.9 * market_factor + rnorm(252, 0, 0.015),
    finance_stock = 0.7 * market_factor + rnorm(252, 0, 0.018),
    energy_stock = 0.4 * market_factor + rnorm(252, 0, 0.025),
    healthcare_stock = 0.6 * market_factor + rnorm(252, 0, 0.013)
  )

# Your task: Create correlation matrix, identify clusters, visualize relationships
```

<br>

**Level 8 Practice**: Climate data with outliers and missing values
```{r}
#| label: practice-project-8
#| eval: false
# Generate messy climate data
climate_study <- tibble(
  station_id = rep(1:20, each = 36),  # 20 stations, 3 years monthly
  month = rep(rep(1:12, 3), 20),
  year = rep(rep(2021:2023, each = 12), 20),
  temperature = rep(rnorm(20, 15, 5), each = 36) + 
                10 * sin(2 * pi * (month - 3) / 12) + 
                rnorm(720, 0, 2),
  precipitation = pmax(0, 100 + 50 * sin(2 * pi * (month - 6) / 12) - 
                      2 * temperature + rnorm(720, 0, 20))
) %>%
  # Add measurement errors and missing data
  mutate(
    extreme_event = rbinom(720, 1, 0.05),
    temperature = ifelse(extreme_event == 1, temperature + rnorm(sum(extreme_event), 0, 10), temperature),
    missing_data = rbinom(720, 1, 0.1),
    temperature = ifelse(missing_data == 1, NA, temperature),
    precipitation = ifelse(missing_data == 1, NA, precipitation)
  )

# Your task: Handle missing data, identify outliers, use robust methods
```

<br>

### Session Info

```{r}
#| label: session-info
sessionInfo()
```

---

*Each level builds on the previous ones, like learning to drive - first you learn to steer, then add gas/brakes, then navigation, then highway driving!*
