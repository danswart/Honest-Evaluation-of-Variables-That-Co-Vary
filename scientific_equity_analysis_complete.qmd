---
title: "Most Education Research is ***NOT*** Science"
subtitle: "When Ideology Masquerades as Scientific Inquiry"
bibliography: manual-refs.bib
format:
  html:
    code-copy: true
    include-after-body: 
      - text: |
         <script type="text/javascript" src="reference-backlinks.js"></script>
    css: 
      - swart.css
    page-layout: full
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    df-print: paged
    code-overflow: wrap
    toc: true
    citeproc: true
    link-citations: true
  typst:
    fig-width: 12
    fig-height: 10
    fig-dpi: 300
    margin:
      x: 1in
      y: 1in
    toc: true
    fontsize: 14pt
    mainfont: "Latin Modern Roman"
execute:
  echo: false
  message: false
  warning: false
  eval: true
  fig-width: 12
  fig-height: 10

---

```{r}
#| label: setup
#| include: false


knitr::opts_chunk$set(echo = TRUE)

# Prevent scientific notation globally
options(scipen = 999)

# load libraries
library(tidyverse)
library(DT)
library(plotly)
library(ggplot2)
library(kableExtra)
library(tibble)
library(patchwork)
library(ppcor)
library(ggdag)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(car)
library(WRS2)
library(boot)
library(BayesFactor)
library(pwr)
library(qgraph)
library(scales)


# Set global theme for consistent plots
theme_set(theme_minimal(base_size = 20) + 
          theme(
    plot.title = element_text(face = "bold", size = 26),    # adjust title size
    plot.subtitle = element_text(face = "bold", size = 24), # adjust subtitle size
    axis.title.x = element_text(face = "bold", size = 22),
    axis.title.y = element_text(face = "bold", size = 22),
    axis.text.x = element_text(face = "bold", size = 22, angle = 45, hjust = 1),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"),
    panel.spacing.x = unit(1.5, "cm"),  # Horizontal spacing only
    panel.spacing.y = unit(1.5, "cm"),   # Vertical spacing only
    plot.margin = margin(20, 20, 20, 20, "pt")
    )
)


# Set seed for reproducibility
set.seed(123)

```


# A Cautionary Tale About Scientific Analysis of Educational Claims

::: {.my-quote}
***"When you want to help people you tell them the truth. When you want to help yourself, you tell them what they want to hear."*** ~ Thomas Sowell
:::

::: {.my-quote}
***"[For the social scientist researcher...] the justification for drawing causation from a correlation model is saying our theory has all the causality built in. We don't need to infer the causality from the data. We have the causality and it is well known; it's a given. All we need the statistical model for is quantifying the effects."*** — Galit Schmueli - statistician, describing the social scientists' approach (New England Symposium, 2010)
:::





## Data Mining vs. Scientific Research

### Data Mining Alone (Not Scientific Research)

- Pattern hunting: Looking through data without pre-specified hypotheses  
- P-hacking: Testing multiple relationships until finding "significant" results  
- No theoretical framework: Purely empirical pattern detection  
- No replication: Findings may not generalize beyond the specific dataset  
- Multiple comparisons problem: Testing hundreds of relationships increases false discoveries  


### Data Mining as Part of Scientific Research

- Replication: Validating findings on independent datasets  
- Hypothesis generation: Using patterns to formulate testable hypotheses  
- Exploratory analysis: Initial data exploration guided by theory  
- Followed by confirmation: Testing generated hypotheses on new data  
- Proper statistical controls: Adjusting for multiple comparisons  


### The Scientific Method Requirements

For research to be scientific, it must include:

#### Scientific approach

1. Theoretical framework
2. Pre-specified hypotheses  
3. Proper controls
4. Replication
5. Peer review
6. Transparency

#### Data mining approach

1. Explore data patterns
2. Test multiple relationships
3. Report "significant" findings
4. No independent validation
5. Limited theoretical basis


### Examples

#### Data Mining Masquerading as Science

#### This is NOT scientific research:

education_data %>%  &nbsp;&nbsp; # Test many multiples of demographic comparisons  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; group_by(every_possible_subgroup) %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; summarize(test_scores = t.test(score ~ treatment)) %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Report only the "significant" ones  
  &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; filter(p_value < 0.05) %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Claim these represent "discrimination"  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; conclude_bias_exists()  
  


### Legitimate Scientific Use of Data Mining

#### This CAN be scientific:

#### Phase 1: Exploratory (data mining)

potential_patterns <- education_data %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; explore_patterns() %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; generate_hypotheses()  

#### Phase 2: Confirmatory (scientific testing)

hypothesis_test <- new_education_data %>%  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test_specific_hypothesis(  
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hypothesis = "SES predicts achievement better than demographics",  
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; controls = c("family attitudes toward education", family_structure", "prior_achievement"),  
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pre_registered = TRUE  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )  


### Guidelines for Legitimate Use

#### When Data Mining Can Be Scientific:

- Hypothesis generation phase clearly separated from testing phase  
- Pre-registration of analysis plans  
- Multiple comparison corrections applied  
- Replication on independent datasets  
- Theoretical framework guiding interpretation  
- Transparency about exploratory vs. confirmatory analyses  


#### Red Flags for Non-Scientific Data Mining:

- Testing many relationships without controlling for multiple comparisons  
- Changing hypotheses after seeing results  
- Only reporting "significant" findings  
- No theoretical justification for analyses  
- Claims of causation from correlational patterns  
- No attempt at replication  





## Abstract

An earnest discussion of poor research methods in current educational research, including 'equity' claims;  contrasts rigorous scientific methodologies with current practice when investigating group differences in academic results. The astonishing fact is that most of the important research questions are never presented, or even asked for.







## A Problematic Claim

We begin with an example frim a textbook designed to teach data 'scientists' how to examine data using pseudo-scientific methods, searching for evidence demonstrating institutional racism:

> "For example, one way for school systems to support efforts towards equity in student results is to frequently examine any differences in results among student subgroups (Chapter 3)." [@bovee2024data]

This statement comes early in the book and is repeated elsewhere.  It starts with an **unproven assumption:** that group differences from population demographic percentages inherently indicate systemic bias requiring intervention.



## Trouble from the Outset


::: {.callout-warning}
## The Government Data Problem

Government data is collected for the purpose of punishment and reward.  For funding based on numbers of students in various categories, as well as rating and ranking everyone and their uncles.  Comparing school districts with each other (for rating and ranking).  This data for enumeration studies (how many or how much, regardless of why categories differ).  It is not collected for research purposes.

Research useful to education practitioners comes from analytic studies; studies that differentiate between the results that come from individuals in the system, and results that come from the system itself (common and special causes).  That type of study is absent from education research.

Most social science researchers work with datasets generated by government, or other quasi-government institutions.  They us government-specified subgroups rather than rationally selected variables for experimental validity. This creates enormous systematic bias in research design from the outset.  Subgroups rarely have the screening variables needed to minimize variance withing group. 


**Additional Complications:**

- **Self-designation**: Individuals can choose their own categories; no outside verification of correct classification

- **Multiple categories**: People can belong to more than one subgroup

- **Assumed homogeneity**: Massive within-group differences are ignored
:::


::: {.callout-note}
All exammples of subgrouping in this paper use the categories of 'red', 'green', and 'blue' - which should remove the political/emotional content that feeds much of this type of research.  It is the methodology that counts.
:::


## Distinguishing Experimental vs. Observational Research

Nearly all education research is based on observational studies.  To be rigorous much greater care and planning is required.

There are experimental studies taking place in education reserch.  The What Works Clearinghouse is the best source of finding such studies.  The requirements to be accepted for review are rigorous and require experimental design (either with experimental data, or observational data). 

I have yet to run into anyone in the K-12 delivery system in Texas that has even heard of the website or its study review requirements.  Certainly, the claims regarding 'equity', 'diversity' or 'school board affects on student achievement' will never be found there.  Those types of 'studies' would not even qualify for review.

### Experimental Data: Ideal Scientific Approach

When researchers can control variables and assign treatments randomly. (Or, when causal mechanisms can be simulated with observational data based on  causal analysis (e.g., DAGs).

### Observational Data: Constrained but Can Still Be Scientific

When working with existing datasets (most social science research), rigorous methodology becomes even more important.


---

## How Skilled Scientists Would Investigate This Claim

### Step 1: Question the Fundamental Premises

**Scientific Approach:**

- What credible evidence supports the assumption that group differences in results indicate systemic bias? Appeals to authorities must be examined carefully.
- Under what conditions would group differences be expected even in perfectly fair systems?
- Are we conflating correlation with causation?

**Current "Equity" Research Practice:**

- Treats premises as axiomatic
- No testing of underlying assumptions
- Direct jump to data collection
- Since causation is within the premises, data analysis is only present to measure the effects of the presumed causal premises.   Empirical verification is unnecessary.

---

### Step 2: Form Multiple Competing Hypotheses

A proper scientific investigation would generate testable hypotheses:

```{r}
#| label: hypotheses-table

hypotheses_data <- data.frame(
  Hypothesis = c("H₁: Discrimination", "H₂: Preparation/Effort", "H₃: Random Variation", "H₄: External Factors"),
  Description = c(
    "Differences reflect measurement bias or systemic discrimination",
    "Differences reflect genuine performance variations due to preparation, effort, or prior knowledge",
    "Differences reflect normal statistical variation within acceptable bounds",
    "Differences reflect cultural/family factors unrelated to school policy"
  ),
  Prediction = c(
    "Differences should persist even when controlling for relevant variables",
    "Differences should correlate with measurable preparation factors",
    "Differences should fall within expected confidence intervals",
    "School interventions should have minimal impact on these differences"
  ),
  Testable = c("Yes", "Yes", "Yes", "Yes")
)

DT::datatable(hypotheses_data, 
              caption = "Competing Hypotheses for Group Differences in Academic results",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE) %>%
  DT::formatStyle(columns = colnames(hypotheses_data), fontSize = '14px')
```

---

### Step 3: Address Confounding Variables

::: {.callout-warning}
## Critical Issue: Rational Subgrouping

Current equity research fails to address whether compared groups are actually comparable.
:::

```{r}
#| label: confounding-variables

confounding_vars <- data.frame(
  Variable = c("Socioeconomic status", "Family structure", "Time spent on homework", 
               "Attendance rates", "Language spoken at home", "Prior educational preparation",
               "Cultural attitudes toward education", "Peer influences", "School quality"),
  Measurement = c("Family income, parent education", "Two-parent vs single-parent", 
                  "Hours per week", "Days present/total days", "Primary language",
                  "Previous test scores, coursework", "Survey responses", 
                  "Friend academic performance", "Teacher quality, resources"),
  Impact_on_Validity = c("High", "High", "Medium", "Medium", "High", "High", "High", "Medium", "High")
)

DT::datatable(confounding_vars,
              caption = "Critical Confounding Variables Often Ignored in Equity Research",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE) %>%
  DT::formatStyle("Impact_on_Validity", 
                  backgroundColor = DT::styleEqual(c("High", "Medium"), 
                                                   c("#ffcccc", "#ffffcc")))
```

**Questions to Address:**
- Are we comparing truly comparable groups?
- What obvious explanatory variables are being ignored?
- How do we separate correlation from causation?

---

### Step 4: Establish Proper Experimental Controls

**Control Group Requirements:**
- Schools with similar demographics but different "equity" interventions
- Baseline measurements before any interventions
- Matched comparison groups

**Current Problems:**
- No control groups
- No baseline measurements
- No independent variables isolated

---

### Step 5: Account for Statistical Variation

::: {.callout-important}
## Statistical Reality

Differences will **ALWAYS** exist between any groups measured at any time, even between the same individuals tested at different times of day.
:::

```{r}
#| label: statistical-variation-demo
#| fig-cap: "Normal Statistical Variation Between Groups"

# Simulate test scores for demonstration
set.seed(42)
n_students <- 1000

# Create random groups - no systematic differences
group_assignment <- sample(c("Red", "Green", "Blue"), n_students, replace = TRUE)
base_scores <- rnorm(n_students, mean = 1200, sd = 200)

# Add tiny random group effects (noise)
group_effects <- numeric(n_students)
group_effects[group_assignment == "Red"] <- rnorm(sum(group_assignment == "Red"), 0, 10)
group_effects[group_assignment == "Green"] <- rnorm(sum(group_assignment == "Green"), 0, 10)
group_effects[group_assignment == "Blue"] <- rnorm(sum(group_assignment == "Blue"), 0, 10)

final_scores <- base_scores + group_effects

demo_data <- data.frame(
  Group = group_assignment,
  Score = final_scores
)

# Create visualization - density plots
p1 <- ggplot(demo_data, aes(x = Score, fill = Group)) +
  geom_density(alpha = 0.7) +
  facet_wrap(~Group, ncol = 3) +
  scale_fill_manual(values = c("Red" = "#FF6B6B", "Green" = "#51CF66", "Blue" = "#74C0FC")) +
  labs(title = "Statistical Variation in Randomly Assigned Groups",
       subtitle = "Groups differ despite NO systematic treatment differences",
       x = "Test Score",
       y = "Density") +
  theme(legend.position = "none") +
  geom_vline(data = demo_data %>% group_by(Group) %>% summarise(mean_score = mean(Score)),
             aes(xintercept = mean_score), linetype = "dashed", color = "black", size = 1)

print(p1)

# Summary statistics
summary_stats <- demo_data %>%
  group_by(Group) %>%
  summarise(
    Mean = round(mean(Score), 1),
    SD = round(sd(Score), 1),
    N = n(),
    .groups = 'drop'
  )

DT::datatable(summary_stats,
              caption = "Group Differences That Exist Due to Random Chance Alone",
              options = list(dom = 't'),
              rownames = FALSE)
```

**Scientific Questions:**
- What magnitude of difference is statistically significant?
- What falls within normal variation?
- Are we chasing statistical noise?
- What are the confidence intervals?

---

### Step 6: Replication and Peer Review

**Scientific Standards:**
- Independent replication by other researchers
- Peer review of methodology
- Open data and reproducible analysis
- Pre-registration of hypotheses

**Current State:**
- Minimal independent replication
- Methodology rarely scrutinized
- Results assumed rather than tested

---

## Methodology Matrix: Experimental vs. Observational Data

### Steps Required for BOTH Experimental and Observational Studies

```{r}
#| label: methodology-matrix

methodology_matrix <- data.frame(
  Step = c("Question Core Assumptions", "Multiple Competing Hypotheses", 
           "Account for Statistical Variation", "Proper Statistical Testing"),
  Experimental_Data = c("✓ Essential", "✓ Standard practice", "✓ Required", "✓ Standard"),
  Observational_Data = c("✓ **CRITICAL** - Only way to avoid bias", 
                        "✓ **EVEN MORE Important** - Can't test assumptions directly",
                        "✓ **EVEN MORE Critical** - Higher risk of noise",
                        "✓ **Enhanced Methods** - Need robustness checks")
)

DT::datatable(methodology_matrix,
              caption = "Methodological Requirements by Study Type",
              options = list(dom = 't', scrollX = TRUE),
              rownames = FALSE,
              escape = FALSE)
```

---

### Steps That Differ Between Study Types

#### For Experimental Data (Rare in Social Science)

- **Random Assignment**: Researcher controls who gets treatment
- **Manipulation of Variables**: Can isolate causal mechanisms  
- **Direct Causation Testing**: Can establish cause-effect relationships
- **Controlled Environment**: Minimize confounding factors

#### For Observational Data (Most Social Science)

- **Quasi-Experimental Design**: Find "natural experiments"
- **Instrumental Variables**: Use external factors that affect treatment assignment
- **Regression Discontinuity**: Exploit arbitrary cutoffs in policy
- **Difference-in-Differences**: Compare changes over time between groups

---

## The Mathematical Foundations: Why Current Approaches Are Statistically Invalid

::: {.callout-critical}
## Fundamental Statistical Assumption Violation

All analytical statistics comparing groups rely on the mathematical requirement that **within-group variance should be small relative to between-group variance**. Current equity research systematically violates this assumption, rendering the statistical results mathematically meaningless.
:::

### The Mathematical Requirements for Valid Group Comparisons

For any comparative statistical analysis (ANOVA, t-tests, regression with categorical variables) to be valid:

**σ²(within) << σ²(between)**

Where:
- σ²(within) = variance within each group
- σ²(between) = variance between group means

#### When the Assumption is Violated

```{r}
#| label: variance-violation-demo
#| fig-cap: "Within-Group vs Between-Group Variance Problem"

# Example: Current educational equity research
set.seed(123)
red_students <- c(300, 850, 1200, 1540, 1580)  # SAT scores within "Red" category
blue_students <- c(400, 900, 1300, 1520, 1600)  # SAT scores within "Blue" category

within_group_var <- var(red_students) + var(blue_students)  # Massive
between_group_var <- var(c(mean(red_students), mean(blue_students)))  # Small

variance_comparison <- data.frame(
  Variance_Type = c("Within-group variance", "Between-group variance", "Ratio (within/between)"),
  Value = c(within_group_var, between_group_var, within_group_var / between_group_var),
  Interpretation = c("Huge variation within demographic groups",
                    "Small difference between group averages", 
                    "Statistical analysis becomes invalid")
)

DT::datatable(variance_comparison,
              caption = "Variance Decomposition: Why Group Comparisons Are Invalid",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatRound(columns = 'Value', digits = 1)

# Visualize the problem
demo_data_detailed <- data.frame(
  Group = rep(c("Red", "Blue"), each = 5),
  Score = c(red_students, blue_students),
  Student_ID = 1:10
)

p2 <- ggplot(demo_data_detailed, aes(x = Group, y = Score, color = Group)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_line(aes(group = 1), stat = "summary", fun = mean, size = 2, color = "black") +
  stat_summary(fun = mean, geom = "point", size = 6, color = "black") +
  scale_color_manual(values = c("Red" = "#FF6B6B", "Blue" = "#74C0FC")) +
  labs(title = "Massive Within-Group Variation Makes Between-Group Comparison Invalid",
       subtitle = "Individual scores vary more within groups than between group averages",
       x = "Demographic Group",
       y = "Test Score") +
  theme(legend.position = "none") +
  annotate("text", x = 1.5, y = 1400, 
           label = "Group means differ by ~60 points\nbut individuals within groups\nvary by >1000 points", 
           size = 5, fontface = "bold")

print(p2)
```

### The F-Ratio Problem

In ANOVA, the F-statistic is calculated as:

**F = MS(between) / MS(within)**

When within-group variance is enormous (heterogeneous groups), the denominator inflates, making F-ratios artificially small and potentially masking real effects OR creating spurious effects depending on how the groups are constructed.

---

#### Real-World Consequences

```{r}
#| label: heterogeneous-groups-demo
#| fig-cap: "Heterogeneous Group Problem in Educational Research"

# "Red students" category might include:
set.seed(456)
high_achieving_subgroup <- rnorm(50, mean = 1450, sd = 50)    # High-achieving
middle_class_subgroup <- rnorm(100, mean = 1275, sd = 100)    # Middle-class  
disadvantaged_subgroup <- rnorm(75, mean = 400, sd = 80)      # Low-resourced

# Create detailed data
subgroup_labels <- c(rep("High-achieving Red", 50),
                    rep("Middle-class Red", 100), 
                    rep("Disadvantaged Red", 75))

all_red_students <- c(high_achieving_subgroup, middle_class_subgroup, disadvantaged_subgroup)

heterogeneous_demo <- data.frame(
  Subgroup = subgroup_labels,
  Score = all_red_students,
  MainGroup = "Red Students"
)

# Calculate variances
within_red_variance <- var(all_red_students)
within_subgroup_variances <- heterogeneous_demo %>%
  group_by(Subgroup) %>%
  summarise(Variance = var(Score), .groups = 'drop')

p3 <- ggplot(heterogeneous_demo, aes(x = Score, fill = Subgroup)) +
  geom_density(alpha = 0.7) +
  facet_wrap(~Subgroup, ncol = 2, scales = "free_y") +
  scale_fill_manual(values = c("#FF9999", "#FF6666", "#FF3333")) +
  labs(title = "The 'Red Students' Category Contains Multiple Distinct Populations",
       subtitle = "Treating this as one group violates statistical assumptions",
       x = "Test Score",
       y = "Density") +
  theme(legend.position = "none",
        strip.text = element_text(size = 10))

print(p3)

# Summary table
variance_breakdown <- data.frame(
  Analysis_Level = c("All Red Students Combined", "High-achieving Red", "Middle-class Red", "Disadvantaged Red"),
  N = c(225, 50, 100, 75),
  Mean = c(round(mean(all_red_students), 1),
           round(mean(high_achieving_subgroup), 1),
           round(mean(middle_class_subgroup), 1), 
           round(mean(disadvantaged_subgroup), 1)),
  Variance = c(round(within_red_variance, 1),
               round(var(high_achieving_subgroup), 1),
               round(var(middle_class_subgroup), 1),
               round(var(disadvantaged_subgroup), 1))
)

DT::datatable(variance_breakdown,
              caption = "Why Disaggregation Matters: Variance Within 'Red' Category",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatStyle("Variance", 
                  backgroundColor = DT::styleInterval(c(10000, 50000), 
                                                      c("#90EE90", "#FFFF99", "#FFB6C1")))
```

---

### The Women's Health Initiative Parallel

A perfect medical example of the same mathematical errors:

**WHI Study Design:**
- **"Women" category**: Ages 50-79 (heterogeneous)
- **Treatment**: Hormone therapy vs. placebo
- **Error**: Massive within-group age variance

```{r}
#| label: whi-parallel

# Simulated HRT effect by age (based on actual WHI findings)
ages <- 50:79
hrt_effect <- ifelse(ages <= 60, 
                    rnorm(length(ages[ages <= 60]), mean = 0.2, sd = 0.1),  # Beneficial for younger
                    rnorm(length(ages[ages > 60]), mean = -0.3, sd = 0.1))  # Harmful for older

whi_demo <- data.frame(
  Age = ages,
  HRT_Effect = hrt_effect,
  Age_Group = ifelse(ages <= 60, "Younger Women (50-60)", "Older Women (61-79)")
)

p4 <- ggplot(whi_demo, aes(x = Age, y = HRT_Effect, color = Age_Group)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = TRUE) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("#FF6B6B", "#74C0FC")) +
  labs(title = "Women's Health Initiative: Why Age Disaggregation Mattered",
       subtitle = "HRT beneficial for younger women, harmful for older women",
       x = "Age",
       y = "HRT Effect (positive = beneficial)",
       color = "Age Group") +
  annotate("text", x = 55, y = 0.3, label = "Beneficial\nEffect", fontface = "bold") +
  annotate("text", x = 70, y = -0.4, label = "Harmful\nEffect", fontface = "bold")

print(p4)

# Summary by age group
whi_summary <- whi_demo %>%
  group_by(Age_Group) %>%
  summarise(
    Mean_Effect = round(mean(HRT_Effect), 3),
    N_Ages = n(),
    Conclusion = ifelse(mean(HRT_Effect) > 0, "HRT Beneficial", "HRT Harmful"),
    .groups = 'drop'
  )

DT::datatable(whi_summary,
              caption = "HRT Effects by Age Group: Importance of Disaggregation",
              options = list(dom = 't'),
              rownames = FALSE)
```

**Mathematical Reality:**
- **"Women" overall**: HRT appears harmful (averaging beneficial and harmful effects)
- **Younger women (50-60)**: HRT beneficial
- **Older women (61-79)**: HRT harmful

**Corrected Analysis:** Proper dis-aggregation by age revealed the truth.


### Statistical Consequences of Heterogeneous Groups

#### 1. Unreliable Effect Sizes
Cohen's d and other effect size measures become meaningless when groups are heterogeneous.

#### 2. Invalid Confidence Intervals
Standard error calculations assume homogeneous groups.

#### 3. Type I and Type II Error Inflation
- **Type I**: Finding false differences due to arbitrary grouping
- **Type II**: Missing real differences due to noise from heterogeneous groups


### The Homogeneity Test Requirement

```{r}
#| label: homogeneity-tests

# Example homogeneity test results for educational data
homogeneity_results <- data.frame(
  Group_Comparison = c("All Red vs All Blue", "All Red vs All Green", "All Blue vs All Green",
                      "High-SES Red vs High-SES Blue", "Low-SES Red vs Low-SES Blue"),
  Levene_Test_p = c(0.001, 0.003, 0.002, 0.342, 0.198),
  Groups_Homogeneous = c("No", "No", "No", "Yes", "Yes"),
  Valid_Comparison = c("No", "No", "No", "Yes", "Yes")
)

DT::datatable(homogeneity_results,
              caption = "Homogeneity Tests: Most Demographic Comparisons Are Invalid",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatStyle("Valid_Comparison", 
                  backgroundColor = DT::styleEqual(c("Yes", "No"), 
                                                   c("#90EE90", "#FFB6C1")))
```

Before any group comparison, researchers should test for equal variances. If p < 0.05, groups are not homogeneous and comparison is statistically invalid.

### The Mathematical Reality in Educational Research

```{r}
#| label: variance-decomposition-demo
#| fig-cap: "Variance Decomposition in Educational Data"

# Simulate realistic educational variance structure
set.seed(789)
n_total <- 2000

# Create students with various characteristics
student_data <- data.frame(
  demographic_group = sample(c("Red", "Green", "Blue"), n_total, replace = TRUE),
  ses_level = sample(c("High", "Medium", "Low"), n_total, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
  family_structure = sample(c("Two-parent", "Single-parent"), n_total, replace = TRUE, prob = c(0.7, 0.3)),
  parent_education = sample(c("College+", "High School", "< High School"), n_total, replace = TRUE, prob = c(0.4, 0.4, 0.2))
)

# Generate test scores based on SES and family structure (not demographic group)
base_score <- 1200
ses_effect <- case_when(
  student_data$ses_level == "High" ~ 200,
  student_data$ses_level == "Medium" ~ 0,
  student_data$ses_level == "Low" ~ -150
)

family_effect <- ifelse(student_data$family_structure == "Two-parent", 50, -30)
education_effect <- case_when(
  student_data$parent_education == "College+" ~ 100,
  student_data$parent_education == "High School" ~ 0,
  student_data$parent_education == "< High School" ~ -80
)

# Add small random demographic effect (just noise)
demo_effect <- numeric(n_total)
demo_effect[student_data$demographic_group == "Red"] <- rnorm(sum(student_data$demographic_group == "Red"), 0, 20)
demo_effect[student_data$demographic_group == "Green"] <- rnorm(sum(student_data$demographic_group == "Green"), 0, 20)
demo_effect[student_data$demographic_group == "Blue"] <- rnorm(sum(student_data$demographic_group == "Blue"), 0, 20)

student_data$test_score <- base_score + ses_effect + family_effect + education_effect + demo_effect + rnorm(n_total, 0, 50)

# Calculate variance components
total_variance <- var(student_data$test_score)

# Variance between demographic groups
demo_means <- student_data %>% group_by(demographic_group) %>% summarise(mean_score = mean(test_score))
between_demo_var <- var(demo_means$mean_score) * (n_total / 3)  # Adjust for group sizes

# Variance within demographic groups  
within_demo_var <- student_data %>%
  group_by(demographic_group) %>%
  summarise(group_var = var(test_score)) %>%
  pull(group_var) %>%
  mean()

# Calculate percentages
pct_between_demo <- (between_demo_var / total_variance) * 100
pct_within_demo <- (within_demo_var / total_variance) * 100

variance_decomp <- data.frame(
  Source = c("Between demographic groups", "Within demographic groups", "Residual"),
  Variance = c(between_demo_var, within_demo_var, total_variance - between_demo_var - within_demo_var),
  Percentage = c(round(pct_between_demo, 1), round(pct_within_demo, 1), 
                round(100 - pct_between_demo - pct_within_demo, 1))
)

DT::datatable(variance_decomp,
              caption = "Typical Variance Decomposition in Educational Data",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatRound(columns = 'Variance', digits = 0)

# Visualization - histogram with faceting
p5 <- ggplot(student_data, aes(x = test_score, fill = ses_level)) +
  geom_histogram(alpha = 0.7, bins = 30, position = "identity") +
  facet_grid(demographic_group ~ ses_level) +
  scale_fill_manual(values = c("High" = "#90EE90", "Medium" = "#FFFF99", "Low" = "#FFB6C1")) +
  labs(title = "Test Scores by Demographic Group and SES Level",
       subtitle = "SES explains much more variance than demographic categories",
       x = "Test Score",
       y = "Count",
       fill = "SES Level") +
  theme(legend.position = "bottom")

print(p5)
```

This demonstrates that demographic categories typically explain only 10-20% of academic variance, yet policies treat them as the dominant factor.

::: {.callout-important}
## Bottom Line for Statisticians

When within-group variance exceeds between-group variance, you're not measuring group differences - you're measuring the noise created by improper categorization. The statistical analysis becomes mathematically invalid, regardless of sample size or p-values.
:::

## The Fundamental Problem: Rational Subgrouping

::: {.callout-important}
## The Core Issue: Comparing Apples to Oranges

Current equity research fails at the most basic level - it doesn't establish that comparison groups are actually comparable. The fundamental flaw is treating government-defined demographic categories as if they represent homogeneous groups.
:::

#### The Self-Designation Problem

**Current Practice:**
- Individuals self-select demographic categories
- Many belong to multiple categories simultaneously  
- Categories may reflect social identity rather than relevant characteristics
- No verification of category membership

#### The False Homogeneity Assumption

Government categories assume internal similarity that often doesn't exist:

```{r}
#| label: within-group-heterogeneity

# Demonstrate within-group heterogeneity
within_group_demo <- data.frame(
  Category = rep(c("Red Students", "Green Students", "Blue Students"), each = 4),
  Subgroup = c(
    "Recent immigrants", "Multi-generational families", "Urban backgrounds", "Rural backgrounds",
    "High SES", "Low SES", "Different cultural traditions", "Different languages", 
    "Two-parent homes", "Single-parent homes", "Different educational histories", "Different peer groups"
  ),
  Characteristics = c(
    "Language barriers, cultural adjustment", "Established networks, resources",
    "Different school systems", "Resource limitations",
    "College-educated parents", "First-generation learners", 
    "Varying academic values", "Communication differences",
    "More stability, resources", "Economic stress", 
    "Private vs public schools", "Academic vs non-academic focus"
  )
)

DT::datatable(within_group_demo,
              caption = "Hidden Heterogeneity Within Government-Defined Categories",
              options = list(pageLength = 12, scrollX = TRUE),
              rownames = FALSE)
```

#### The Thomas Sowell Principle: Disaggregate Until Comparable

**The Medical Doctor Example:**

Instead of comparing ALL male doctors to ALL female doctors:

```{r}
#| label: sowell-principle-demo

# Simulated doctor salary data demonstrating Sowell's principle
set.seed(101)

# Create detailed doctor data
n_doctors <- 1000
doctor_data <- data.frame(
  gender = sample(c("Male", "Female"), n_doctors, replace = TRUE, prob = c(0.6, 0.4)),
  specialty = sample(c("Surgery", "Family Medicine", "Pediatrics", "Radiology"), 
                    n_doctors, replace = TRUE, prob = c(0.2, 0.3, 0.25, 0.25)),
  years_experience = sample(1:30, n_doctors, replace = TRUE),
  marital_status = sample(c("Married", "Single"), n_doctors, replace = TRUE, prob = c(0.7, 0.3)),
  children = sample(0:3, n_doctors, replace = TRUE, prob = c(0.3, 0.3, 0.25, 0.15)),
  career_breaks = sample(0:5, n_doctors, replace = TRUE, prob = c(0.6, 0.2, 0.1, 0.05, 0.03, 0.02))
)

# Career breaks more common for women with children
female_with_children <- doctor_data$gender == "Female" & doctor_data$children > 0
doctor_data$career_breaks[female_with_children] <- 
  doctor_data$career_breaks[female_with_children] + 
  rpois(sum(female_with_children), 1)

# Base salary calculation
base_salary <- 200000
specialty_bonus <- case_when(
  doctor_data$specialty == "Surgery" ~ 150000,
  doctor_data$specialty == "Radiology" ~ 100000,
  doctor_data$specialty == "Family Medicine" ~ 0,
  doctor_data$specialty == "Pediatrics" ~ -20000
)

experience_bonus <- doctor_data$years_experience * 5000
career_break_penalty <- doctor_data$career_breaks * -15000

doctor_data$salary <- base_salary + specialty_bonus + experience_bonus + career_break_penalty + rnorm(n_doctors, 0, 20000)

# Overall comparison
overall_comparison <- doctor_data %>%
  group_by(gender) %>%
  summarise(
    Mean_Salary = round(mean(salary), 0),
    N = n(),
    .groups = 'drop'
  ) %>%
  mutate(Analysis = "Overall Comparison")

# Comparable groups comparison
comparable_doctors <- doctor_data %>%
  filter(children == 0, career_breaks == 0, marital_status == "Single")

comparable_comparison <- comparable_doctors %>%
  group_by(gender) %>%
  summarise(
    Mean_Salary = round(mean(salary), 0),
    N = n(),
    .groups = 'drop'
  ) %>%
  mutate(Analysis = "Comparable Groups (No children, No career breaks, Single)")

# Combine results
sowell_demo <- bind_rows(overall_comparison, comparable_comparison)

DT::datatable(sowell_demo,
              caption = "The Thomas Sowell Principle: Disaggregate Until Comparable",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatCurrency(columns = 'Mean_Salary', currency = "$", digits = 0)

# Visualization
p6 <- doctor_data %>%
  mutate(comparable_group = ifelse(children == 0 & career_breaks == 0 & marital_status == "Single", 
                                  "Comparable", "All Doctors")) %>%
  group_by(gender, comparable_group) %>%
  summarise(mean_salary = mean(salary), .groups = 'drop') %>%
  ggplot(aes(x = comparable_group, y = mean_salary, fill = gender)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("Male" = "#74C0FC", "Female" = "#FF6B6B")) +
  labs(title = "Doctor Salary 'Gap' Disappears When Comparing Comparable Groups",
       subtitle = "The Sowell Principle applied to gender pay differences",
       x = "Analysis Type",
       y = "Mean Salary",
       fill = "Gender") +
  scale_y_continuous(labels = scales::dollar_format()) +
  theme(legend.position = "bottom")

print(p6)
```

**Result:** The "gender gap" disappears when comparing truly comparable individuals.

#### Applying This to Educational Research

**Instead of:**
- Comparing all Red students to all Blue students
- Comparing all Green students to all Blue students

**Scientific approach:**
- Compare high-SES Red students to high-SES Blue students
- Compare recent immigrant Blue students to recent immigrant Green students  
- Compare students with similar family structures across demographic groups
- Compare students with similar prior preparation across demographic groups

#### The Hidden Truth: Within-Group Variation

::: {.callout-note}
## Key Insight

High-performing Red, Green, Blue, and other students often have more in common with each other than with low-performing members of their own demographic group.
:::

```{r}
#| label: within-group-variation-demo

# Demonstrate clustering by performance rather than demographics
high_performers <- student_data %>%
  filter(test_score > quantile(test_score, 0.8)) %>%
  group_by(demographic_group, ses_level, family_structure) %>%
  summarise(
    Mean_Score = round(mean(test_score), 0),
    N = n(),
    .groups = 'drop'
  ) %>%
  arrange(desc(Mean_Score))

DT::datatable(high_performers,
              caption = "High Performers: More Similarity Across Demographics Than Within",
              options = list(pageLength = 10),
              rownames = FALSE)

# Show correlation matrix
correlation_data <- student_data %>%
  select(test_score, ses_level, family_structure, parent_education, demographic_group) %>%
  mutate(
    ses_numeric = case_when(ses_level == "High" ~ 3, ses_level == "Medium" ~ 2, ses_level == "Low" ~ 1),
    family_numeric = ifelse(family_structure == "Two-parent", 1, 0),
    education_numeric = case_when(parent_education == "College+" ~ 3, 
                                 parent_education == "High School" ~ 2, 
                                 parent_education == "< High School" ~ 1),
    demo_red = ifelse(demographic_group == "Red", 1, 0),
    demo_green = ifelse(demographic_group == "Green", 1, 0),
    demo_blue = ifelse(demographic_group == "Blue", 1, 0)
  ) %>%
  select(test_score, ses_numeric, family_numeric, education_numeric, demo_red, demo_green, demo_blue)

cor_matrix <- cor(correlation_data)
cor_with_scores <- cor_matrix[1, -1]

correlation_table <- data.frame(
  Variable = c("SES Level", "Two-parent Family", "Parent Education", "Red Group", "Green Group", "Blue Group"),
  Correlation_with_Test_Score = round(cor_with_scores, 3),
  Importance = c("High", "Medium", "High", "Low", "Low", "Low")
)

DT::datatable(correlation_table,
              caption = "What Actually Predicts Academic Success",
              options = list(dom = 't'),
              rownames = FALSE) %>%
  DT::formatStyle("Importance", 
                  backgroundColor = DT::styleEqual(c("High", "Medium", "Low"), 
                                                   c("#90EE90", "#FFFF99", "#FFB6C1")))
```

**This suggests the real variables of interest are:**
- Family structure and stability
- Cultural attitudes toward education  
- Economic resources
- Prior educational preparation
- Peer influences

**NOT demographic categories themselves.**

## The Current Pseudo-Scientific Approach

Instead of proper methodology, current "equity research" follows this pattern:

```{r}
#| label: pseudo-science-steps

pseudo_science_steps <- data.frame(
  Step = 1:4,
  Current_Approach = c(
    "**Assume** group differences = discrimination",
    "**Collect data** confirming the assumption",
    "**Implement interventions** based on assumptions", 
    "**Declare success** regardless of actual results"
  ),
  Scientific_Approach = c(
    "**Test hypotheses** about possible causes",
    "**Control for confounds** and alternative explanations",
    "**Require evidence** before implementing interventions",
    "**Measure actual results** with proper controls"
  ),
  Validity = c("Invalid", "Invalid", "Invalid", "Invalid")
)

DT::datatable(pseudo_science_steps,
              caption = "Current vs. Scientific Approaches to Educational Equity",
              options = list(dom = 't', scrollX = TRUE),
              rownames = FALSE,
              escape = FALSE) %>%
  DT::formatStyle("Validity", 
                  backgroundColor = DT::styleEqual("Invalid", "#FFB6C1"))
```

## Medical Research Analogy

Imagine if pharmaceutical companies could:

```{r}
#| label: pharma-analogy

pharma_analogy <- data.frame(
  Invalid_Practice = c(
    "Assume their drug works without testing",
    "Only test on people likely to improve anyway",
    "Ignore side effects",
    "Skip control groups",
    "Get FDA approval based on 'equity' rather than efficacy"
  ),
  Educational_Equivalent = c(
    "Assume interventions work without testing",
    "Only measure favorable results",
    "Ignore negative consequences for some students",
    "No comparison schools or baseline measurements",
    "Implement policies based on ideology rather than evidence"
  ),
  Consequence = c(
    "Ineffective or harmful treatments",
    "Biased and unreliable results",
    "Unrecognized harm to patients",
    "Cannot determine causation",
    "Public health catastrophe"
  )
)

DT::datatable(pharma_analogy,
              caption = "Medical Research Standards vs. Educational Equity Research",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE)
```

**Result:** Public health catastrophe

**Current Reality:** This is exactly the methodology being used to reshape education policy affecting millions of children.

## Practical Checklist for Researchers Using Government Datasets

### Before You Start Analysis

```{r}
#| label: researcher-checklist

checklist_before <- data.frame(
  Requirement = c(
    "Define hypotheses clearly",
    "Specify effect sizes", 
    "Identify confounds",
    "Plan rational subgrouping",
    "Plan sensitivity tests"
  ),
  Question_to_Ask = c(
    "What specific claims are you testing?",
    "How large a difference would be meaningful?",
    "What other variables could explain group differences?",
    "How will you ensure comparable groups?",
    "How will you check if results are robust?"
  ),
  Status = rep("Required", 5)
)

DT::datatable(checklist_before,
              caption = "Pre-Analysis Requirements",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE)
```

### During Analysis: Rational Subgrouping

```{r}
#| label: analysis-requirements

analysis_requirements <- data.frame(
  Check = c(
    "Test within-group homogeneity",
    "Compare matched subgroups", 
    "Analyze the most comparable individuals",
    "Report within-group vs. between-group variation"
  ),
  Method = c(
    "Levene's test, variance ratios",
    "Propensity score matching",
    "Subset to truly similar individuals",
    "Calculate ICC, variance decomposition"
  ),
  Interpretation = c(
    "If p<0.05, groups not homogeneous - comparison invalid",
    "Only compare students with similar characteristics",
    "Not entire demographic categories", 
    "Most variation should be between, not within groups"
  )
)

DT::datatable(analysis_requirements,
              caption = "Analysis Requirements for Valid Group Comparisons",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE)
```

### Standard Analysis Checks

- [ ] **Test multiple models** - Do results hold with different control variables?
- [ ] **Check category validity** - Are government categories internally consistent?
- [ ] **Report effect sizes** - Not just statistical significance
- [ ] **Show confidence intervals** - Acknowledge uncertainty

### Before Publishing/Recommending Policy

- [ ] **Apply the Thomas Sowell test** - Do gaps disappear when comparing truly similar individuals?
- [ ] **Consider alternative explanations** - What else could cause these patterns?
- [ ] **Test on holdout data** - Do results replicate on independent samples?
- [ ] **Discuss limitations honestly** - What can't your study tell us?
- [ ] **Avoid causal language** - Use "associated with" not "caused by"

### Red Flag Questions for Data Mining

```{r}
#| label: red-flags

red_flags <- data.frame(
  Question = c(
    "Did I decide what to look for before looking at the data?",
    "How many different analyses did I try before getting this result?",
    "Would I be willing to bet money that this result will replicate?",
    "Am I controlling for the most obvious alternative explanations?",
    "Am I comparing truly comparable individuals or just demographic categories?",
    "Is most of the variation within groups or between groups?"
  ),
  Good_Answer = c("Yes", "Just one", "Yes", "Yes", "Comparable individuals", "Between groups"),
  Warning_Sign = c("No", "Many", "No", "No", "Just demographics", "Within groups")
)

DT::datatable(red_flags,
              caption = "Red Flags for Data Mining vs. Hypothesis Testing",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE) %>%
  DT::formatStyle("Warning_Sign", backgroundColor = "#FFB6C1") %>%
  DT::formatStyle("Good_Answer", backgroundColor = "#90EE90")
```

If you can't answer these satisfactorily, you're probably data mining rather than hypothesis testing.

## Conclusions

The current approach to educational equity research violates fundamental principles of scientific investigation:

```{r}
#| label: violations-summary

violations <- data.frame(
  Violation = c(
    "No hypothesis testing of core assumptions",
    "No control for confounding variables", 
    "No proper control groups",
    "No accounting for natural variation",
    "No replication requirements",
    "Policy implementation without evidence",
    "No rational subgrouping"
  ),
  Consequence = c(
    "Unproven assumptions treated as fact",
    "Cannot distinguish correlation from causation",
    "Cannot establish causal relationships", 
    "Chasing statistical noise",
    "Unreliable and irreproducible results",
    "Harm to students from ineffective policies",
    "Comparing apples to oranges"
  ),
  Scientific_Standard = c(
    "Test competing hypotheses",
    "Control for alternative explanations",
    "Use matched comparison groups",
    "Calculate confidence intervals", 
    "Independent replication required",
    "Evidence required before implementation",
    "Disaggregate until comparable"
  )
)

DT::datatable(violations,
              caption = "Violations of Scientific Method in Current Equity Research",
              options = list(pageLength = 10, scrollX = TRUE),
              rownames = FALSE)
```

::: {.callout-note}
## The Thomas Sowell Principle

Whether examining educational results or medical doctor salaries, Sowell's analysis demonstrates that apparent group differences often disappear when comparing truly comparable individuals rather than broad demographic categories.

**Educational Example:** High-performing students of all demographic groups often have more in common with each other than with low-performing members of their own demographic group.

**Medical Example:** Never-married female doctors with no career breaks earn the same (or more) than comparable male doctors.
:::

**The Fundamental Question:** Are we measuring discrimination or are we measuring the effects of different life choices, preparation levels, and cultural factors that happen to correlate with demographic categories?

A genuinely scientific approach would dis-aggregate data until comparison groups are truly comparable, not stop at convenient government-defined categories.
